{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be7cffd",
   "metadata": {},
   "source": [
    "# Pré-Processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9534e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bem vindo ao Machine Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Lista de pacotes necessários e os nomes para importar\n",
    "pacotes = {\n",
    "    \"pandas\": \"pd\",\n",
    "    \"numpy\": \"np\",\n",
    "    \"seaborn\": \"sns\",\n",
    "    \"matplotlib\": \"plt\",\n",
    "    \"IPython\": None,\n",
    "    \"scikit-learn\": None,\n",
    "    \"tabulate\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e335df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para instalar pacotes\n",
    "def instalar(pacote):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pacote])\n",
    "\n",
    "# Verifica e instala os pacotes\n",
    "for pacote, alias in pacotes.items():\n",
    "    try:\n",
    "        importlib.import_module(pacote)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {pacote}...\")\n",
    "        instalar(pacote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d28f1",
   "metadata": {},
   "source": [
    "## Bibliotecas usadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b54323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a biblioteca pandas, amplamente utilizada para análise e manipulação de dados em Python.\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# O módulo 'os' fornece funções para interagir com o sistema operacional,\n",
    "# como manipulação de arquivos, diretórios e variáveis de ambiente.\n",
    "import os\n",
    "\n",
    "# O módulo 'tarfile' permite ler e escrever arquivos compactados no formato .tar, .tar.gz, .tgz, etc.\n",
    "# É útil para extrair ou criar arquivos compactados.\n",
    "import tarfile\n",
    "\n",
    "# O módulo 'urllib' oferece funções para manipular URLs e fazer requisições HTTP,\n",
    "# como baixar arquivos da internet.\n",
    "import urllib\n",
    "\n",
    "# Importa a biblioteca Seaborn, que é utilizada para criação de gráficos estatísticos,\n",
    "# tornando a visualização de dados mais simples e visualmente agradável.\n",
    "import seaborn as sns\n",
    "\n",
    "# Importa o módulo pyplot da biblioteca Matplotlib, que fornece funções para gerar gráficos\n",
    "# como linha, dispersão, barras, histogramas, entre outros, e permite controle total sobre eles.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Importando a biblioteca necessária\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import necessário\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Normalizar os dados antes de aplicar KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbbb84",
   "metadata": {},
   "source": [
    "## Obtenção dos Dados\n",
    "\n",
    "Pacotes usados:\n",
    "\n",
    "```python\n",
    "# Importa a biblioteca pandas, amplamente utilizada para análise e manipulação de dados em Python.\n",
    "import pandas as pd\n",
    "\n",
    "# O módulo 'os' fornece funções para interagir com o sistema operacional,\n",
    "# como manipulação de arquivos, diretórios e variáveis de ambiente.\n",
    "import os\n",
    "\n",
    "# O módulo 'tarfile' permite ler e escrever arquivos compactados no formato .tar, .tar.gz, .tgz, etc.\n",
    "# É útil para extrair ou criar arquivos compactados.\n",
    "import tarfile\n",
    "\n",
    "# O módulo 'urllib' oferece funções para manipular URLs e fazer requisições HTTP,\n",
    "# como baixar arquivos da internet.\n",
    "import urllib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5d66f",
   "metadata": {},
   "source": [
    "Para este fase da oficina trabalharemos com um conjunto de dados dos imóveis em distritos da Califórnia, considerando uma série de características desses distritos, cada instância é um distrito. Basta fazer o download do arquivo de um arquivo compactado, *housing.tgz*, que contém o arquivo *housing.csv*.\n",
    "\n",
    "Você poderia usar o navegador para baixar o arquivo, mas é preferível criar uma pequena função para tal. Ter uma função para baixar arquivos é uma boa prática, pois você pode reutilizá-la em outros projetos. Além disso, você pode adicionar funcionalidades extras, como verificar se o arquivo já foi baixado ou não.\n",
    "\n",
    "As funções abaixo fazem o download do arquivo, descompactam o arquivo e carregam os dados em um DataFrame do Pandas. O arquivo CSV contém informações sobre os preços de imóveis na Califórnia, incluindo características como número de quartos, idade da casa, localização e outros fatores que podem influenciar o preço. Esses dados são frequentemente usados em análises de preços de imóveis e modelos preditivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c021af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml/master/'\n",
    "# Define a URL base de onde os dados serão baixados.\n",
    "\n",
    "HOUSING_PATH = os.path.join('datasets', 'housing')\n",
    "# Cria um caminho local ('datasets/housing') para armazenar os dados baixados.\n",
    "# Usa os.path.join para garantir compatibilidade entre sistemas operacionais.\n",
    "\n",
    "HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'\n",
    "# Monta a URL completa do arquivo compactado que será baixado,\n",
    "# juntando a URL base com o caminho do arquivo no repositório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \"\"\"\n",
    "    Função responsável por:\n",
    "    1. Criar uma pasta local para armazenar os dados (se ela não existir).\n",
    "    2. Baixar um arquivo compactado (.tgz) de um repositório online.\n",
    "    3. Extrair esse arquivo para a pasta especificada.\n",
    "\n",
    "    Parâmetros:\n",
    "    - housing_url: URL onde está localizado o arquivo compactado com os dados.\n",
    "    - housing_path: Caminho local onde os dados serão salvos e extraídos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cria o diretório onde os dados serão armazenados.\n",
    "    # Se o diretório já existir, 'exist_ok=True' evita que um erro seja gerado.\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "\n",
    "    # Define o caminho completo onde o arquivo .tgz será salvo localmente.\n",
    "    # Junta o caminho da pasta com o nome do arquivo.\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
    "\n",
    "    # Faz o download do arquivo a partir da URL especificada.\n",
    "    # Salva o arquivo compactado no caminho definido por 'tgz_path'.\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "\n",
    "    # Abre o arquivo compactado (.tgz) para leitura.\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "\n",
    "    # Extrai todo o conteúdo do arquivo compactado para a pasta definida.\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "\n",
    "    # Fecha o arquivo .tgz para liberar recursos do sistema.\n",
    "    housing_tgz.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e790a",
   "metadata": {},
   "source": [
    "Quando chamamos `fetch_housing_data()` cria um diretório *datasets/housing* em seu workspace, baixa o arquivo *housing.tgz* e extrai o arquivo *housing.csv* nesse diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd4361",
   "metadata": {},
   "source": [
    "Quando chamamos `fetch_housing_data()`, ele cria um diretório chamado `datasets/housings` em seu workspace, faz o download do arquivo *housing.tgz* e extrai o arquivo *housing.csv* para esse diretório. Agora carregaremos carregaremos os dados com o Pandas. Mais uma vez, você deve descrever uma pequena função para carregá-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    \"\"\"\n",
    "    Função responsável por carregar os dados de habitação (housing) que estão armazenados em um arquivo CSV.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - housing_path: Caminho da pasta onde o arquivo 'housing.csv' está localizado.\n",
    "    \n",
    "    Retorna:\n",
    "    - Um DataFrame do pandas contendo os dados carregados do arquivo CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define o caminho completo do arquivo CSV, unindo o caminho da pasta com o nome do arquivo.\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "\n",
    "    # Usa a função read_csv do pandas para ler o arquivo CSV localizado em 'csv_path'.\n",
    "    # Retorna o conteúdo como um DataFrame, que é uma estrutura de dados tabular (tabelas) muito poderosa no pandas.\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2d6c5",
   "metadata": {},
   "source": [
    "Vamos olhar as 5 primeiras linhas do DataFrame que iremos carregar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(housing.columns):\n",
    "    print(f'Variável {i+1}: {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107e5f9",
   "metadata": {},
   "source": [
    "---\n",
    "**Descrição das Variáveis do Dataset Housing:**\n",
    "\n",
    "- **Variável 1:** `longitude` — Longitude da localização.\n",
    "- **Variável 2:** `latitude` — Latitude da localização.\n",
    "- **Variável 3:** `housing_median_age` — Mediana da idade das construções residenciais naquela área.\n",
    "- **Variável 4:** `total_rooms` — Número total de cômodos (rooms) nas residências da área.\n",
    "- **Variável 5:** `total_bedrooms` — Número total de quartos nas residências da área.\n",
    "- **Variável 6:** `population` — População da área.\n",
    "- **Variável 7:** `households` — Número de domicílios (households) na área.\n",
    "- **Variável 8:** `median_income` — Renda mediana dos moradores da área (em dezenas de milhares de dólares).\n",
    "- **Variável 9:** `median_house_value` — Valor mediano das residências naquela área (em dólares).\n",
    "- **Variável 10:** `ocean_proximity` — Proximidade com o oceano (categorias como \"INLAND\", \"NEAR OCEAN\", etc.).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa394f89",
   "metadata": {},
   "source": [
    "## Pré-Análise Exploratória dos Dados\n",
    "\n",
    "Pacotes usados:\n",
    "\n",
    "```python\n",
    "# Importa a biblioteca Seaborn, que é utilizada para criação de gráficos estatísticos,\n",
    "# tornando a visualização de dados mais simples e visualmente agradável.\n",
    "import seaborn as sns\n",
    "\n",
    "# Importa o módulo pyplot da biblioteca Matplotlib, que fornece funções para gerar gráficos\n",
    "# como linha, dispersão, barras, histogramas, entre outros, e permite controle total sobre eles.\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527413d",
   "metadata": {},
   "source": [
    "### Agora o momento de investigação dos dados 🔍- *MUITA ATENÇÃO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f744e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c65fd",
   "metadata": {},
   "source": [
    "O ``info()`` é bom, entretanto podemos ter uma análise mais minuciosa quanto ao tipo e quantidade de instâncias, vamos criar uma função para isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da classe DataAnalyzer\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Inicializa o analisador com um DataFrame.\n",
    "        \n",
    "        Parâmetros:\n",
    "        - df: pandas DataFrame que será utilizado para análise, logo ele já tem de estar importado no ambiente.\n",
    "        \n",
    "        Atributos criados:\n",
    "        - self.df: guarda o DataFrame passado na criação do objeto.\n",
    "        - self.total: guarda a quantidade total de observações (linhas) do DataFrame.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.total = len(df)\n",
    "\n",
    "    def tipos_variaveis(self):\n",
    "        \"\"\"\n",
    "        Mostra a contagem dos tipos de variáveis presentes no DataFrame.\n",
    "        \n",
    "        Funcionalidade:\n",
    "        - Conta quantas variáveis são de cada tipo (ex.: int64, float64, object, etc.).\n",
    "        - Exibe os resultados formatados visualmente, com separadores e emojis.\n",
    "        \"\"\"\n",
    "        print('='*50)  # Linha de separação\n",
    "        print('📊 Tipos de Variáveis:')  # Título da seção\n",
    "        print('-'*50)  # Linha de separação\n",
    "        print(self.df.dtypes.value_counts())  # Conta e exibe os tipos de dados das colunas\n",
    "        print('='*50)  # Linha de fechamento\n",
    "\n",
    "    def analise_dados_nulos(self):\n",
    "        \"\"\"\n",
    "        Realiza uma análise de dados nulos no DataFrame.\n",
    "        \n",
    "        Funcionalidade:\n",
    "        - Mostra para cada coluna:\n",
    "            - Quantidade de valores não nulos.\n",
    "            - Quantidade de valores nulos.\n",
    "            - Porcentagem de valores não nulos.\n",
    "            - Porcentagem de valores nulos.\n",
    "        - Organiza essas informações em formato tabular, bem alinhado.\n",
    "        \"\"\"\n",
    "        print('='*50)\n",
    "        print('🔍 Análise de Dados Nulos:')\n",
    "        print('-'*50)\n",
    "\n",
    "        # Cria um DataFrame auxiliar com as informações de nulos e não nulos\n",
    "        dados_nulos = pd.DataFrame({\n",
    "            'Não Nulos': self.df.notnull().sum(),  # Quantidade de não nulos por coluna\n",
    "            'Nulos': self.df.isnull().sum(),  # Quantidade de nulos por coluna\n",
    "            'Porcentagem Não Nulos (%)': (self.df.notnull().sum() / self.total * 100).round(2),  # Porcentagem de não nulos\n",
    "            'Porcentagem Nulos (%)': (self.df.isnull().sum() / self.total * 100).round(2)  # Porcentagem de nulos\n",
    "        })\n",
    "\n",
    "        # Exibe o DataFrame de dados nulos em formato string, alinhado como tabela\n",
    "        print(dados_nulos.to_string())  # Essa linha faz com que a tabela fique alinhada horizontalmente, sem quebra\n",
    "        print('='*50)  # Linha de fechamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eabc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing).tipos_variaveis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85b0a5",
   "metadata": {},
   "source": [
    "Vamos enxergar melhor as NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a209dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma nova figura para o gráfico, definindo o tamanho (largura=10, altura=5).\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Gera um heatmap (mapa de calor) utilizando seaborn.\n",
    "# O DataFrame housing.isnull() retorna True para valores nulos e False para não nulos.\n",
    "# Cada quadrado do heatmap representa se há (ou não) um valor nulo na respectiva célula.\n",
    "# cbar=False remove a barra de cores lateral (opcional).\n",
    "# cmap='viridis' define o esquema de cores (pode ser 'viridis', 'magma', 'coolwarm', etc.).\n",
    "# yticklabels=False remove os rótulos dos índices no eixo Y (para deixar o gráfico mais limpo).\n",
    "sns.heatmap(housing.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "\n",
    "# Adiciona um título ao gráfico.\n",
    "plt.title('Heatmap de valores Nulos (NA) no DataFrame')\n",
    "\n",
    "# Define o rótulo (label) do eixo X, que representa as colunas do DataFrame.\n",
    "plt.xlabel('Colunas')\n",
    "\n",
    "# Rotaciona os nomes das colunas no eixo X para 45 graus,\n",
    "# facilitando a leitura quando os nomes são longos ou numerosos.\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf3b8a",
   "metadata": {},
   "source": [
    "A maioria dos algoritmos de aprendizado de máquina não lidam bem com dados ausentes. Portanto, é importante identificar e tratar esses dados antes de prosseguir com a análise ou modelagem. O tratamento de dados ausentes pode incluir a remoção de linhas ou colunas que contêm valores ausentes, ou o preenchimento desses valores com estimativas apropriadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf128e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selecionar colunas numéricas automaticamente\n",
    "numeric_cols = housing.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# 2. Configuração do gráfico\n",
    "n_cols = 3  # Número de colunas no grid\n",
    "n_rows = len(numeric_cols) // n_cols  # Calcula linhas necessárias\n",
    "\n",
    "plt.figure(figsize=(14, 4*n_rows))  # Ajuste automático de altura\n",
    "\n",
    "# 3. Criar histogramas para cada variável\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    # Histograma com KDE\n",
    "    sns.histplot(housing[col], \n",
    "                 bins=30, \n",
    "                 kde=True, \n",
    "                 color='#1f77b4',  # Azul matplotlib\n",
    "                 edgecolor='white',\n",
    "                 linewidth=0.5)\n",
    "    \n",
    "    # Customização\n",
    "    plt.title(f'Distribuição de {col}', fontsize=12, pad=10)\n",
    "    plt.xlabel('Valor', fontsize=10)\n",
    "    plt.ylabel('Frequência', fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar linhas de referência\n",
    "    plt.axvline(housing[col].mean(), color='red', linestyle='--', linewidth=1, label='Média')\n",
    "    plt.axvline(housing[col].median(), color='green', linestyle='-', linewidth=1, label='Mediana')\n",
    "    \n",
    "    if i == 1:  # Legenda apenas no primeiro gráfico\n",
    "        plt.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Espaçamento entre subplots\n",
    "plt.suptitle('Distribuição das Variáveis Numéricas', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f1449",
   "metadata": {},
   "source": [
    "Podemos observar distribuições assimétricas em todos os gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e77597",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5*n_rows))\n",
    "plt.suptitle('Análise de Outliers - Box Plots', y=1.02, fontsize=14)\n",
    "\n",
    "# 3. Criar box plots para cada variável\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    # Box plot customizado\n",
    "    sns.boxplot(y=housing[col], \n",
    "                color='#4C72B0', \n",
    "                width=0.5,\n",
    "                flierprops=dict(marker='o', \n",
    "                               markersize=5,\n",
    "                               markerfacecolor='#DD8452',\n",
    "                               markeredgecolor='none',\n",
    "                               alpha=0.7))\n",
    "    \n",
    "    # Adicionar linha da mediana\n",
    "    median = housing[col].median()\n",
    "    plt.axhline(median, color='#55A868', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Customização\n",
    "    plt.title(col, fontsize=12, pad=10)\n",
    "    plt.ylabel('Valores', fontsize=9)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Anotar estatísticas\n",
    "    stats = housing[col].describe()\n",
    "    textstr = f\"Mediana: {median:.2f}\\nQ1: {stats['25%']:.2f}\\nQ3: {stats['75%']:.2f}\"\n",
    "    plt.text(0.05, 0.95, textstr, \n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize=8,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98b901",
   "metadata": {},
   "source": [
    "## Limpeza e Tratamento dos Dados\n",
    "\n",
    "Pacotes usados:\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 📦 Importando a biblioteca necessária\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import necessário\n",
    "from sklearn.model_selection import train_test_split\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea025b",
   "metadata": {},
   "source": [
    "### **Data Imputation**\n",
    "\n",
    "Uma decisão crucial que você deve tomar é o que fazer com os dados ausentes. Você pode optar por remover as linhas ou colunas que contêm valores ausentes, ou pode preencher esses valores com a média, mediana ou outro valor apropriado. A escolha depende do contexto dos dados e do impacto que os valores ausentes podem ter na análise. Para esta oficina, vamos optar por preencher os valores ausentes (NAs) com algum estimador, porém vamos mostrar como remover as linhas ou colunas com NAs também.\n",
    "\n",
    "Antes disso, temos que entender o que é imputação de dados. A imputação de dados é o processo de substituir valores ausentes em um conjunto de dados por valores estimados. Isso é importante porque muitos algoritmos de aprendizado de máquina não podem lidar com dados ausentes e, portanto, a imputação é uma etapa crucial no pré-processamento dos dados.\n",
    "A imputação pode ser feita de várias maneiras, incluindo:\n",
    "- **Média/Mediana/Moda:** Substituir valores ausentes pela média, mediana ou moda da coluna.\n",
    "- **KNNImputer:** Usar o algoritmo K-Nearest Neighbors para prever valores ausentes com base em valores de instâncias semelhantes.\n",
    "- **Regressão:** Usar um modelo de regressão para prever valores ausentes com base em outras variáveis.\n",
    "\n",
    "#### 🎯 Design do Scikit-Learn\n",
    "\n",
    "A API do Scikit-Learn é notavelmente bem projetada. Estes são os principais princípios de design:\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Consistência\n",
    "\n",
    "Todos os objetos compartilham uma interface consistente e simples.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Estimadores**\n",
    "\n",
    "Qualquer objeto que possa estimar alguns parâmetros com base em um conjunto de dados é chamado de **estimador** (por exemplo, um `SimpleImputer` é um estimador).  \n",
    "\n",
    "- A estimativa é realizada pelo método `fit()`, que recebe um conjunto de dados como parâmetro.  \n",
    "- Para algoritmos de aprendizado supervisionado, o `fit()` recebe dois conjuntos de dados: um com as amostras e outro com os rótulos (*labels*).  \n",
    "- Qualquer outro parâmetro necessário para orientar o processo de estimativa é chamado de **hiperparâmetro** (por exemplo, a estratégia do `SimpleImputer`), e deve ser definido como uma variável de instância (geralmente por meio de um parâmetro do construtor).\n",
    "\n",
    "---\n",
    "\n",
    "##### **Transformadores**\n",
    "\n",
    "Alguns estimadores (como o `SimpleImputer`) também podem **transformar** um conjunto de dados; estes são chamados de **transformadores**.\n",
    "\n",
    "- A transformação é feita pelo método `transform()`, que recebe o conjunto de dados a ser transformado e retorna o conjunto transformado.  \n",
    "- Essa transformação geralmente depende dos parâmetros aprendidos durante o `fit()`, como acontece com o `SimpleImputer`.  \n",
    "- Todos os transformadores possuem também o método `fit_transform()`, que é equivalente a chamar `fit()` seguido de `transform()`.  \n",
    "  - **Obs:** em alguns casos, `fit_transform()` é otimizado e executa muito mais rápido do que chamar os dois métodos separadamente.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Preditores**\n",
    "\n",
    "Alguns estimadores são capazes de fazer **previsões**; estes são chamados de **preditores**.\n",
    "\n",
    "- Por exemplo, o modelo `LinearRegression` é um preditor: dado o PIB per capita de um país, ele prevê o nível de satisfação com a vida.  \n",
    "- Um preditor possui o método `predict()`, que recebe um conjunto de novas instâncias e retorna as previsões correspondentes.  \n",
    "- Ele também possui o método `score()`, que mede a qualidade das previsões, dado um conjunto de teste (e os rótulos correspondentes, no caso de aprendizado supervisionado).\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Inspeção\n",
    "\n",
    "- Todos os **hiperparâmetros** de um estimador são acessíveis diretamente via variáveis públicas de instância (por exemplo, `imputer.strategy`).  \n",
    "- Todos os **parâmetros aprendidos** são acessíveis via variáveis públicas de instância com um **sufixo de sublinhado** (por exemplo, `imputer.statistics_`).\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Não proliferação de classes\n",
    "\n",
    "- Conjuntos de dados são representados como arrays do **NumPy** ou matrizes esparsas do **SciPy**, em vez de classes personalizadas.  \n",
    "- Hiperparâmetros são apenas strings ou números comuns do Python.\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Composição\n",
    "\n",
    "- Blocos de construção existentes são reutilizados sempre que possível.  \n",
    "- Por exemplo, é fácil criar um **Pipeline** (fluxo de processamento) a partir de uma sequência arbitrária de transformadores seguida por um estimador final.\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Valores padrão sensatos\n",
    "\n",
    "- O Scikit-Learn fornece valores padrão razoáveis para a maioria dos parâmetros.  \n",
    "- Isso facilita a criação rápida de um sistema funcional básico (*baseline*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc15504",
   "metadata": {},
   "source": [
    "#### 1° Opção: Remover instâncias com NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um novo DataFrame chamado 'housing_not_na' onde as linhas que possuem\n",
    "# valores nulos (NaN) na coluna 'total_bedrooms' são removidas.\n",
    "\n",
    "housing_not_na = housing.dropna(\n",
    "    subset=['total_bedrooms'],  # Define que a verificação de nulos será feita apenas na coluna 'total_bedrooms'.\n",
    "    inplace=False                # inplace=False garante que o DataFrame original (housing) não será modificado,\n",
    "                                 # e sim retornará um novo DataFrame com as linhas sem nulos nessa coluna.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13146aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing_not_na).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9be4b",
   "metadata": {},
   "source": [
    "#### 2° Opção: Remover colunas com NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um novo DataFrame chamado 'housing_not_na' removendo a coluna 'total_bedrooms' do DataFrame original.\n",
    "\n",
    "housing_not_na = housing.drop(\n",
    "    'total_bedrooms',  # Especifica qual coluna será removida. Neste caso, 'total_bedrooms'.\n",
    "    axis=1,             # axis=1 indica que estamos removendo uma COLUNA (se fosse axis=0, seria uma LINHA).\n",
    "    inplace=False       # inplace=False garante que a operação não altera o DataFrame original (housing),\n",
    "                        # mas retorna um novo DataFrame com a coluna removida.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99707e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing_not_na).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79757f",
   "metadata": {},
   "source": [
    "#### 3° Opção: Preencher NAs com a alguma medida de tendência central da coluna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45464627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Calculando a média da coluna 'total_bedrooms'\n",
    "# A função .mean() calcula a média aritmética dos valores numéricos, ignorando automaticamente os valores NaN\n",
    "mean = housing['total_bedrooms'].mean()\n",
    "\n",
    "# ✅ Fazendo uma cópia do dataframe original para não alterar os dados originais\n",
    "housing_mean = housing.copy()\n",
    "\n",
    "# ✅ Substituindo os valores ausentes (NaN) da coluna 'total_bedrooms' pela média calculada\n",
    "# A função .fillna(mean) preenche todas as células que estão com NaN com o valor da média\n",
    "housing_mean['total_bedrooms'] = housing_mean['total_bedrooms'].fillna(mean)\n",
    "\n",
    "# ✅ Verificando quantos valores ausentes ainda existem na coluna 'total_bedrooms' após a imputação\n",
    "# A função .isnull().sum() retorna a quantidade de valores que ainda são NaN (se tudo deu certo, deve ser zero)\n",
    "na_mean = housing_mean['total_bedrooms'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51484351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a mediana da coluna 'total_bedrooms' do DataFrame 'housing'\n",
    "# A mediana é uma medida de tendência central que é menos sensível a valores extremos do que a média.\n",
    "median = housing['total_bedrooms'].median()\n",
    "\n",
    "# Cria uma cópia do DataFrame original 'housing' para não modificar os dados originais\n",
    "housing_median = housing.copy()\n",
    "\n",
    "# Substitui os valores ausentes (NaN) da coluna 'total_bedrooms' pela mediana calculada\n",
    "# O método fillna() preenche valores faltantes, garantindo que não haja dados ausentes nessa coluna.\n",
    "housing_median['total_bedrooms'] = housing_median['total_bedrooms'].fillna(median)\n",
    "\n",
    "# Verifica quantos valores ausentes (NaN) ainda existem na coluna 'total_bedrooms' após a substituição\n",
    "# O método isnull() identifica valores nulos e sum() faz a contagem total deles.\n",
    "na_median = housing_median['total_bedrooms'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída em Markdown\n",
    "display(Markdown(f\"\"\"\n",
    "## 🔧 Resultado da Imputação de Dados Nulos\n",
    "\n",
    "- 🧠 Após preencher com **Média**, restam **{na_mean}** valores nulos na coluna `total_bedrooms`.\n",
    "- 🧠 Após preencher com **Mediana**, restam **{na_median}** valores nulos na coluna `total_bedrooms`.\n",
    "\n",
    "✅ Ambos os métodos resolveram os dados faltantes, caso o número seja zero.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b22900",
   "metadata": {},
   "source": [
    "Esse processo pode ser automatizado com o uso de bibliotecas como o **Scikit-learn**, que fornece uma classe útil que se encarrega de valores ausentes: a ``SimpleImputer``. Essa classe pode ser usada para preencher valores ausentes com a média, mediana ou moda de uma coluna. Vejamos como usá-la: primeiro, você precisa criar uma instância da ``SimpleImputer``, especificando que deseja substituir os valores ausentes de cada atributo pela média/mediana desse atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71118258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696bfd5",
   "metadata": {},
   "source": [
    "A ``imputer`` simplesmente calcula a média de cada coluna e armazena esses valores em sua variável ``statistics_``. Somente o atributo `total_bedrooms` tem valores ausentes, então o imputer calcula a média apenas desse atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.mean().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2641c08",
   "metadata": {},
   "source": [
    "Agora, você pode usar o método ``transform()`` para preencher os valores ausentes com a média de cada coluna. O método ``transform()`` retorna um novo array NumPy, que contém os dados preenchidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c31528",
   "metadata": {},
   "source": [
    "Você pode usar o método ``fit_transform()`` para fazer isso em uma única etapa também:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = imputer.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0759c7",
   "metadata": {},
   "source": [
    "#### 4° Opção: Preencher NAs com a estimação de um modelo\n",
    "\n",
    "para essa caso usaremos o KNNImputer do Scikit-learn. O KNNImputer é uma técnica de imputação que utiliza o algoritmo K-Nearest Neighbors (KNN) para preencher valores ausentes em um conjunto de dados. Ele funciona identificando os vizinhos mais próximos de uma instância com dados ausentes e usando os valores desses vizinhos para estimar o valor ausente. Essa abordagem é útil quando os dados têm uma estrutura espacial ou temporal, pois considera a similaridade entre as instâncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b28ed",
   "metadata": {},
   "source": [
    "#### 🔢 Demonstração Matemática do KNN e do KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510f572",
   "metadata": {},
   "source": [
    "##### 📚 **Teoria do KNN (K-Nearest Neighbors)**\n",
    "\n",
    "O algoritmo KNN é um método baseado em instâncias usado tanto para **classificação quanto regressão**, onde uma amostra desconhecida é classificada ou recebe um valor estimado com base nos seus **K vizinhos mais próximos** no espaço de características.\n",
    "\n",
    "A base matemática do KNN se fundamenta na ideia de **distâncias** no espaço vetorial, mais frequentemente utilizando a **Distância Euclidiana**, embora outras métricas também possam ser aplicadas (Manhattan, Minkowski, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "##### 🔗 **Equação da Distância Euclidiana**\n",
    "\n",
    "Para dois pontos $X = (x_1, x_2, ..., x_n)$ e $Y = (y_1, y_2, ..., y_n)$ em um espaço $n$-dimensional, a distância euclidiana é calculada como:\n",
    "\n",
    "$\n",
    "d(X, Y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$\n",
    "\n",
    "Essa é a métrica padrão no KNN, pois mede a \"reta\" que liga dois pontos no espaço.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🚩 **Processo Matemático do KNN**\n",
    "\n",
    "1. Calcular a distância entre o ponto com valor desconhecido e todos os outros pontos do conjunto de dados.\n",
    "   \n",
    "2. Ordenar os dados com base na menor distância.\n",
    "\n",
    "3. Selecionar os **K vizinhos mais próximos**.\n",
    "\n",
    "4. - **Regressão:** Calcular a **média** dos valores da variável alvo dos K vizinhos:\n",
    "\n",
    "   $\n",
    "   \\hat{y} = \\dfrac{\\sum_{i=1}^{K} y_i}{K}\n",
    "   $\n",
    "\n",
    "   - **Classificação:** Selecionar a **classe mais frequente** entre os vizinhos.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🔧 **Teoria do KNNImputer para Dados Faltantes**\n",
    "\n",
    "O **KNNImputer** aplica exatamente o mesmo conceito do KNN, mas ao invés de prever uma variável alvo externa, ele preenche os valores **faltantes nas próprias colunas do dataset**.\n",
    "\n",
    "- Para cada célula com valor ausente:\n",
    "  1. Localizam-se os **K registros mais próximos** (com base nas outras colunas que têm valores presentes).\n",
    "  2. Calcula-se a **média dos valores** dos vizinhos na coluna com dados ausentes.\n",
    "  3. Substitui-se o valor nulo pela média calculada.\n",
    "\n",
    "---\n",
    "\n",
    "##### ⚙️ **Fórmula da Imputação**\n",
    "\n",
    "Dado um valor ausente na coluna $j$ do ponto $x$, o valor imputado é:\n",
    "\n",
    "$\n",
    "\\hat{x}_j = \\frac{\\sum_{i=1}^{K} x_{ij}}{K}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $x_{ij}$ = valor da coluna $j$ no vizinho $i$.\n",
    "- $K$ = número de vizinhos considerados.\n",
    "\n",
    "O processo é repetido para cada valor ausente, considerando as distâncias calculadas **apenas nas colunas que não possuem valores ausentes simultaneamente**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f84064",
   "metadata": {},
   "source": [
    "#### ✍️ **Exemplo Manual (Feito na mão)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d182772",
   "metadata": {},
   "source": [
    "Imagine um dataset simplificado com 3 registros e 3 variáveis ($A$, $B$ e $C$):\n",
    "\n",
    "| Registro | A   | B   | C   |\n",
    "|----------|-----|-----|-----|\n",
    "| 1        | 1.0 | 2.0 | 5.0 |\n",
    "| 2        | 2.0 | NaN | 7.0 |\n",
    "| 3        | 3.0 | 6.0 | 9.0 |\n",
    "\n",
    "---\n",
    "\n",
    "##### ✔️ **Passo 1:** Queremos imputar o valor faltante na linha 2, coluna **B**.\n",
    "\n",
    "---\n",
    "\n",
    "##### ✔️ **Passo 2:** Calculamos a distância da linha 2 para as outras linhas utilizando as colunas **A** e **C**, que estão completas.\n",
    "\n",
    "- **Distância para linha 1:**\n",
    "\n",
    "$\n",
    "d = \\sqrt{(2 - 1)^2 + (7 - 5)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.24\n",
    "$\n",
    "\n",
    "- **Distância para linha 3:**\n",
    "\n",
    "$\n",
    "d = \\sqrt{(2 - 3)^2 + (7 - 9)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.24\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "##### ✔️ **Passo 3:** Selecionamos os $K = 2$ vizinhos mais próximos (linha 1 e linha 3).\n",
    "\n",
    "---\n",
    "\n",
    "##### ✔️ **Passo 4:** Pegamos os valores da coluna **B** dos vizinhos:\n",
    "\n",
    "- Linha 1 → **B = 2.0**\n",
    "- Linha 3 → **B = 6.0**\n",
    "\n",
    "---\n",
    "\n",
    "##### ✔️ **Passo 5:** Calculamos a média dos vizinhos:\n",
    "\n",
    "$\n",
    "\\hat{B} = \\frac{2.0 + 6.0}{2} = 4.0\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "##### ✔️ **Resultado:** O valor **NaN** na linha 2, coluna **B**, será preenchido com **4.0**.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🔥 **Vantagens do KNNImputer**\n",
    "\n",
    "- ✅ Leva em consideração a estrutura dos dados.\n",
    "- ✅ Mais robusto do que imputação por média ou mediana simples.\n",
    "- ✅ Considera relações não lineares entre as variáveis.\n",
    "\n",
    "---\n",
    "\n",
    "##### ⚠️ **Desvantagens**\n",
    "\n",
    "- 🚫 Alto custo computacional em datasets grandes.\n",
    "- 🚫 Sensível à escolha de $K$ (um $K$ muito pequeno ou muito grande pode distorcer os resultados).\n",
    "- 🚫 Dependente da escala das variáveis (precisa de **normalização ou padronização** antes de aplicar, devido à distância Euclidiana ser sensível à escala).\n",
    "\n",
    "---\n",
    "\n",
    "##### 💡 **Observação Importante**\n",
    "\n",
    "✔️ Antes de aplicar o **KNNImputer**, é **fundamental normalizar ou padronizar os dados**, pois as distâncias podem ser distorcidas se as variáveis estiverem em escalas muito diferentes.\n",
    "\n",
    "---\n",
    "\n",
    "##### 📖 **Conclusão**\n",
    "\n",
    "O KNNImputer é uma técnica poderosa e intuitiva de imputação que funciona bem quando há relações estruturais nos dados. Entretanto, precisa ser usada com cuidado em relação à escolha de $K$ e à normalização dos dados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403245a5",
   "metadata": {},
   "source": [
    "#### Continuação da 4° Opção\n",
    "\n",
    "Agora vamos **aplicar o `KNNImputer`** ao nosso conjunto de dados.  \n",
    "\n",
    "O `KNNImputer` é uma técnica de imputação que preenche valores ausentes com base na média (ou outro critério) dos **valores mais próximos** — ou seja, ele busca os **k vizinhos mais próximos** de cada amostra incompleta e utiliza esses vizinhos para estimar o valor faltante.  \n",
    "\n",
    "Dessa forma, a imputação leva em conta a **similaridade entre as amostras**, resultando em uma abordagem mais robusta do que simplesmente substituir por médias ou medianas globais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83593cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Selecionando apenas variáveis numéricas do dataframe\n",
    "# Isso é necessário porque o KNNImputer trabalha apenas com variáveis numéricas\n",
    "data = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "# ✅ Separando os dados em dois conjuntos:\n",
    "# 1. Dados COM valores na coluna 'total_bedrooms' (CNA = Complete No NA)\n",
    "#    -> Usado para treino e validação do modelo de imputação\n",
    "data_cna = data.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# 2. Dados COM valores ausentes na coluna 'total_bedrooms'\n",
    "#    -> Este será o conjunto onde aplicaremos o modelo treinado para imputar os NAs reais\n",
    "data_na = data[data['total_bedrooms'].isna()]\n",
    "\n",
    "# ✅ Verificando o tamanho dos dois conjuntos\n",
    "print(f\"Shape dos dados completos (CNA): {data_cna.shape}\")\n",
    "print(f\"Shape dos dados com NA em total_bedrooms: {data_na.shape}\")\n",
    "\n",
    "# ✅ Reduzindo o dataset completo (CNA) para 8.000 observações de forma aleatória\n",
    "# Isso é útil para acelerar o processamento e testes\n",
    "# np.random.seed(42) define uma semente aleatória para garantir que os resultados sejam reproduzíveis\n",
    "np.random.seed(42)\n",
    "data_cna_reduzido = data_cna.sample(n=8000)\n",
    "\n",
    "# ✅ Confirmando o tamanho do dataset reduzido\n",
    "print(f\"Shape dos dados CNA reduzidos: {data_cna_reduzido.shape}\")\n",
    "\n",
    "# ✅ Definindo as variáveis para a modelagem:\n",
    "# X -> todas as variáveis independentes (exceto 'total_bedrooms')\n",
    "# y -> variável dependente, que será imputada (total_bedrooms)\n",
    "X = data_cna_reduzido.drop(columns=['total_bedrooms'])\n",
    "y = data_cna_reduzido['total_bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Elbow Method com Validação Cruzada (4 Folds) para KNN Imputer\n",
    "\n",
    "# Lista para armazenar os RMSE médios de cada valor de K testado\n",
    "rmse_values = []\n",
    "\n",
    "# Definindo os valores de K que serão testados (de 1 a 10)\n",
    "k_values = range(1, 11)\n",
    "\n",
    "# 🔄 Definindo o método de Validação Cruzada:\n",
    "# - n_splits=4 → divide o dataset em 4 partes\n",
    "# - shuffle=True → embaralha os dados antes de dividir (garante aleatoriedade)\n",
    "# - random_state=42 → fixa a semente para garantir resultados reproduzíveis\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop externo: testa cada valor de K (número de vizinhos)\n",
    "for k in k_values:\n",
    "    fold_rmse = []  # Lista para armazenar os RMSE de cada fold (validação)\n",
    "\n",
    "    # Loop interno: executa a validação cruzada (4 folds)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # 🔧 Separando os dados de treino e teste com base nos índices dos folds\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # 🛠️ Criando os dataframes de treino e teste com a coluna alvo 'total_bedrooms'\n",
    "        # Dados de treino possuem 'total_bedrooms' conhecido\n",
    "        train_data = X_train.copy()\n",
    "        train_data['total_bedrooms'] = y_train\n",
    "\n",
    "        # Dados de teste simulam NA na coluna alvo (como se precisássemos imputar)\n",
    "        test_data = X_test.copy()\n",
    "        test_data['total_bedrooms'] = np.nan\n",
    "\n",
    "        # 🔗 Concatenando treino + teste para que o KNN Imputer busque os vizinhos no conjunto inteiro\n",
    "        combined = pd.concat([train_data, test_data])\n",
    "\n",
    "        # 🔥 Escalonamento:\n",
    "        # - O KNN é sensível às escalas das variáveis.\n",
    "        # - StandardScaler padroniza os dados para média 0 e desvio padrão 1.\n",
    "        scaler = StandardScaler()\n",
    "        combined_scaled = scaler.fit_transform(combined)\n",
    "\n",
    "        # 🚀 Aplicando KNN Imputer:\n",
    "        # - n_neighbors=k → número de vizinhos considerados para imputação\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed = imputer.fit_transform(combined_scaled)\n",
    "\n",
    "        # 🔍 Separando os dados de teste imputados:\n",
    "        # - imputed[-len(X_test):, -1]:\n",
    "        #   -> Seleciona as últimas 'len(X_test)' linhas do array 'imputed'.\n",
    "        #   -> O índice negativo -len(X_test) significa: \"comece a selecionar a partir desta posição contando de trás pra frente\".\n",
    "        #   -> Isso garante que estamos pegando exatamente as linhas do conjunto de teste, assumindo que ele foi concatenado no final.\n",
    "        #   -> O -1 seleciona a última coluna ('total_bedrooms').\n",
    "        imputed_test = imputed[-len(X_test):, -1]\n",
    "\n",
    "        # ✅ Calculando o RMSE (Root Mean Squared Error - Erro Quadrático Médio):\n",
    "        # - Antes de comparar as previsões (imputed_test) com os valores reais (y_test),\n",
    "        #   é necessário DESFAZER o escalonamento que foi aplicado anteriormente com o StandardScaler.\n",
    "        # \n",
    "        # - Fórmula para reverter o StandardScaler:\n",
    "        #     valor_original = valor_escalado * desvio_padrao + media\n",
    "        #\n",
    "        # - Aqui usamos:\n",
    "        #   scaler.scale_[-1]: seleciona o desvio padrão da última coluna ('total_bedrooms').   \n",
    "        #   scaler.mean_[-1]: seleciona a média da última coluna ('total_bedrooms').\n",
    "        #\n",
    "        # ✅ Por que usamos o índice -1?\n",
    "        #   -> O índice -1 sempre aponta para o último elemento de uma lista ou array.\n",
    "        #   -> Como 'total_bedrooms' é a última coluna do dataset transformado, pegamos os parâmetros de escalonamento\n",
    "        #      (desvio padrão e média) correspondentes a essa coluna.\n",
    "        #\n",
    "        # ❗ Se usássemos índices positivos, teríamos que contar a posição exata, o que é mais propenso a erro\n",
    "        #     se o número ou a ordem das colunas mudar.\n",
    "        #\n",
    "        # - Após desfazer o escalonamento, calculamos o RMSE para avaliar o erro da imputação.\n",
    "        rmse = np.sqrt(mean_squared_error(\n",
    "            y_test, \n",
    "            imputed_test * scaler.scale_[-1] + scaler.mean_[-1]  # Desfazendo o escalonamento da última coluna\n",
    "        ))\n",
    "\n",
    "        # Armazena o RMSE desse fold\n",
    "        fold_rmse.append(rmse)\n",
    "\n",
    "    # 📊 Após os 4 folds, calcula o RMSE médio para o valor atual de K\n",
    "    avg_rmse = np.mean(fold_rmse)\n",
    "\n",
    "    # Salva o RMSE médio na lista geral\n",
    "    rmse_values.append(avg_rmse)\n",
    "\n",
    "    # Exibe o resultado na tela\n",
    "    print(f\"K={k}: RMSE médio={avg_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Criação do gráfico Elbow Method para visualizar o RMSE em função do número de vizinhos (K)\n",
    "\n",
    "# 🔧 Define o tamanho da figura do gráfico\n",
    "plt.figure(figsize=(10, 6))  # Largura=10, Altura=6\n",
    "\n",
    "# 🪢 Plota os valores de RMSE médio para cada K:\n",
    "# - k_values → eixo X (número de vizinhos)\n",
    "# - rmse_values → eixo Y (erro médio quadrático para cada K)\n",
    "# - marker='o' → adiciona bolinhas nos pontos para destacar\n",
    "plt.plot(k_values, rmse_values, marker='o')\n",
    "\n",
    "# 🎨 Título do gráfico\n",
    "plt.title('Elbow Method para KNNImputer (com Escalonamento)')\n",
    "\n",
    "# 🏷️ Nome dos eixos\n",
    "plt.xlabel('Número de Vizinhos (K)')\n",
    "plt.ylabel('RMSE Médio')\n",
    "\n",
    "# 🗺️ Adiciona uma grade (linhas de referência no fundo do gráfico)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d81084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔎 Encontrando o melhor valor de K baseado no menor RMSE\n",
    "\n",
    "# np.argmin(rmse_values) retorna o índice do menor valor dentro da lista rmse_values\n",
    "# k_values é uma sequência (range) com os valores testados de K\n",
    "# Então, usamos esse índice para acessar o k correspondente no k_values\n",
    "\n",
    "best_k = k_values[np.argmin(rmse_values)]  # Seleciona o K que teve o menor RMSE\n",
    "\n",
    "# Exibe o resultado para o usuário\n",
    "print(f\"Melhor valor de K: {best_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Separando as variáveis numéricas e categóricas\n",
    "# Seleciona apenas as colunas numéricas do dataframe\n",
    "numericas = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "# Seleciona as colunas categóricas que você quer manter para depois juntar (ex: 'ocean_proximity')\n",
    "categoricas = housing[['ocean_proximity']]  # Se houver mais variáveis categóricas, adiciona na lista\n",
    "\n",
    "# 🔧 Separando os dados numéricos:\n",
    "# 'numericas_cna' contém as linhas onde 'total_bedrooms' NÃO tem valores faltantes (completo)\n",
    "numericas_cna = numericas.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# 'numericas_na' contém as linhas onde 'total_bedrooms' está faltando (NaN)\n",
    "numericas_na = numericas[numericas['total_bedrooms'].isna()]\n",
    "\n",
    "# 🔗 Agora concatenamos as duas partes para montar o dataset completo de variáveis numéricas,\n",
    "# onde as linhas com 'total_bedrooms' faltando vão estar no final\n",
    "full_numericas = pd.concat([numericas_cna, numericas_na])\n",
    "\n",
    "# 🔥 Escalonando as variáveis numéricas para que todas fiquem na mesma escala\n",
    "# Isso evita que variáveis com valores muito maiores dominem o cálculo da distância no KNN\n",
    "scaler = StandardScaler()\n",
    "full_scaled = scaler.fit_transform(full_numericas)\n",
    "\n",
    "# 🚀 Aplicando o KNNImputer com o número ótimo de vizinhos (K=4 no exemplo)\n",
    "# O imputador vai preencher os valores faltantes baseando-se nos 4 vizinhos mais próximos\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "imputed_data = imputer.fit_transform(full_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a240e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔙 Desfazendo o escalonamento\n",
    "# imputed_data é um array numpy com os dados após imputação, mas ainda padronizados (z-score)\n",
    "# scaler.scale_ é um array com o desvio padrão de cada coluna calculado no fit do StandardScaler\n",
    "# scaler.mean_ é um array com a média de cada coluna calculada no fit do StandardScaler\n",
    "# Para voltar aos valores originais (antes do escalonamento), aplicamos a fórmula inversa do z-score:\n",
    "# valor_original = valor_padronizado * desvio_padrão + média\n",
    "imputed_data = imputed_data * scaler.scale_ + scaler.mean_\n",
    "\n",
    "\n",
    "# 🔧 Convertendo o resultado imputado (array NumPy) de volta para DataFrame pandas\n",
    "# columns=full_numericas.columns -> mantemos os nomes originais das colunas\n",
    "# index=full_numericas.index -> mantemos os índices originais (linhas)\n",
    "# Isso é importante para manter o alinhamento e facilitar manipulações futuras\n",
    "imputed_numericas = pd.DataFrame(imputed_data, columns=full_numericas.columns, index=full_numericas.index)\n",
    "\n",
    "\n",
    "# ✅ Juntando as variáveis categóricas (não numéricas) que foram separadas antes\n",
    "# pd.concat() concatena DataFrames pelo eixo das colunas (axis=1)\n",
    "# Isso garante que todas as variáveis (numéricas + categóricas) estejam no mesmo DataFrame final\n",
    "imputed_final = pd.concat([imputed_numericas, categoricas], axis=1)\n",
    "\n",
    "\n",
    "# Executa uma análise para verificar se existem valores ausentes (NaN) no DataFrame final\n",
    "# DataAnalyzer é uma classe customizada que tem um método analise_dados_nulos()\n",
    "DataAnalyzer(imputed_final).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏁 Dados finais prontos\n",
    "housing_imputed = imputed_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43cd3a",
   "metadata": {},
   "source": [
    "#### Comparar o desempenho do KNN-Imputer com os datasets NA, média e mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_idx = housing['total_bedrooms'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d89506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices onde total_bedrooms era NA no dataset original\n",
    "na_idx = housing['total_bedrooms'].isna()\n",
    "\n",
    "# Datasets para plotar (com total_bedrooms e median_house_value)\n",
    "datasets = {\n",
    "    'Original (com NA)': housing[['total_bedrooms', 'median_house_value']],\n",
    "    'Imputação pela média': housing_mean[['total_bedrooms', 'median_house_value']],\n",
    "    'Imputação pela mediana': housing_median[['total_bedrooms', 'median_house_value']],\n",
    "    'Imputação KNN': imputed_final[['total_bedrooms', 'median_house_value']],\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i, (name, df) in enumerate(datasets.items(), start=1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    \n",
    "    # Plotar todos os pontos em azul claro\n",
    "    plt.scatter(df['total_bedrooms'], df['median_house_value'], s=10, alpha=0.4, label='Dados Originais')\n",
    "    \n",
    "    # Se for um método que imputou, destacar os valores imputados\n",
    "    if name in ['Imputação pela média', 'Imputação pela mediana', 'Imputação KNN']:\n",
    "        # Pega os dados imputados (nas posições de NA no original)\n",
    "        imputados_x = df.loc[na_idx, 'total_bedrooms']\n",
    "        imputados_y = df.loc[na_idx, 'median_house_value']\n",
    "        \n",
    "        # Plota os pontos imputados em vermelho, maior e com transparência menor para destacar\n",
    "        plt.scatter(imputados_x, imputados_y, color='red', s=30, alpha=0.7, label='Valores Imputados')\n",
    "    \n",
    "    plt.title(name)\n",
    "    plt.xlabel('Total Bedrooms')\n",
    "    plt.ylabel('Median House Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a57e37",
   "metadata": {},
   "source": [
    "### **Transformadores de Dados**\n",
    "\n",
    "Embora o Sklearn tenha muitos transformadores, nós precisamos escrever nossos prórpios transformadores para tarefas como operações de pré-processamento, como normalização, padronização, transformação de variáveis categóricas em variáveis numéricas, etc. Queremos que nosso transformador funcione perfeitamente com as funcionalidades do Sklearn, como Pipelines e GridSearchCV. Para isso, precisamos só precisamos criar uma classe e implementar os métodos ``fit()``, ``transform()`` e ``fit_transform()``. Para obter o último, basta acresentar ``TranformerMixin`` como uma classe. Além disso, se adicionar o ``BaseEstimator`` como uma classe, você terá acesso a todos os métodos de estimadores do Sklearn, como ``get_params()`` e ``set_params()``. Isso é útil para definir os hiperparâmetros do seu transformador.\n",
    "\n",
    "Por exemplo, invés de desenvolver o código anterior do **KNNImputer** de forma manual, você pode usar o transformador do Sklearn para automatizar o processo e tornar ele reutilizável. O código abaixo mostra como fazer isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e16fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedAttributeOther(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col, k_candidates=range(1,11), cv=4):\n",
    "        \"\"\"\n",
    "        Inicializa o imputador customizado.\n",
    "\n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        target_col : str\n",
    "            Nome da coluna alvo que contém valores faltantes a serem imputados.\n",
    "        k_candidates : iterable, default=range(1,11)\n",
    "            Lista ou intervalo com os valores de 'k' vizinhos para testar no KNNImputer.\n",
    "        cv : int, default=4\n",
    "            Número de folds para validação cruzada na busca do melhor 'k'.\n",
    "        \"\"\"\n",
    "        self.target_col = target_col\n",
    "        self.k_candidates = list(k_candidates)\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Ajusta o imputador aos dados, encontrando o melhor valor de 'k' via validação cruzada,\n",
    "        escalonando os dados e treinando o imputador final.\n",
    "\n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Dataset completo contendo a coluna alvo e demais variáveis numéricas.\n",
    "        y : Ignorado (compatibilidade com sklearn)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        self : objeto ajustado\n",
    "        \"\"\"\n",
    "        # Seleciona as colunas numéricas do dataset\n",
    "        self.numeric_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Verifica se a coluna alvo é numérica e está no dataset\n",
    "        if self.target_col not in self.numeric_cols_:\n",
    "            raise ValueError(f\"Coluna alvo '{self.target_col}' deve ser numérica e estar no DataFrame\")\n",
    "\n",
    "        # Usa apenas as linhas onde a coluna alvo não está faltando para treinamento/validação\n",
    "        train_data = X.dropna(subset=[self.target_col])\n",
    "        \n",
    "        # Separa features (numéricas, menos a target) e target\n",
    "        X_train = train_data[self.numeric_cols_].drop(columns=[self.target_col])\n",
    "        y_train = train_data[self.target_col]\n",
    "\n",
    "        # Escalona os dados numéricos para melhorar desempenho do KNN\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        best_k = None\n",
    "        best_score = np.inf  # Inicializa com infinito para buscar mínimo RMSE\n",
    "\n",
    "        # Configura validação cruzada com embaralhamento para robustez\n",
    "        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n",
    "\n",
    "        # Para cada candidato a 'k', avalia o desempenho médio com CV\n",
    "        for k in self.k_candidates:\n",
    "            scores = []\n",
    "            for train_idx, val_idx in kf.split(X_train_scaled):\n",
    "                # Dados treino e validação do fold\n",
    "                X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                # Cria e treina o imputador KNN para o fold com k vizinhos\n",
    "                imputer = KNNImputer(n_neighbors=k)\n",
    "                X_tr_imputed = imputer.fit_transform(np.c_[X_tr, y_tr.values.reshape(-1,1)])\n",
    "\n",
    "                # Imputa valores no conjunto de validação\n",
    "                X_val_imputed = imputer.transform(np.c_[X_val, y_val.values.reshape(-1,1)])\n",
    "\n",
    "                # Pega só a coluna alvo imputada\n",
    "                y_val_pred = X_val_imputed[:, -1]\n",
    "\n",
    "                # Calcula RMSE do fold (erro entre valor original e imputado)\n",
    "                score = np.sqrt(np.mean((y_val.values - y_val_pred)**2))\n",
    "                scores.append(score)\n",
    "\n",
    "            # Média dos scores dos folds para este k\n",
    "            mean_score = np.mean(scores)\n",
    "\n",
    "            # Atualiza melhor k se melhor resultado\n",
    "            if mean_score < best_score:\n",
    "                best_score = mean_score\n",
    "                best_k = k\n",
    "\n",
    "        # Armazena o melhor k encontrado\n",
    "        self.best_k_ = best_k\n",
    "\n",
    "        # Salva o scaler (para uso na transformação)\n",
    "        self.scaler_ = scaler\n",
    "\n",
    "        # Cria imputador final com melhor k encontrado\n",
    "        self.imputer_ = KNNImputer(n_neighbors=best_k)\n",
    "\n",
    "        # Treina o imputador com o dataset completo, escalonado\n",
    "        X_full = X[self.numeric_cols_].drop(columns=[self.target_col])\n",
    "        y_full = X[self.target_col]\n",
    "        X_scaled_full = scaler.transform(X_full)\n",
    "\n",
    "        self.imputer_.fit(np.c_[X_scaled_full, y_full.values.reshape(-1,1)])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Aplica a imputação no dataset X fornecido, substituindo os valores faltantes\n",
    "        da coluna alvo pelos valores imputados pelo KNNImputer treinado.\n",
    "\n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Dataset para transformar/imputar.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        X_new : pd.DataFrame\n",
    "            Dataset com a coluna alvo imputada.\n",
    "        \"\"\"\n",
    "        # Copia as colunas numéricas do dataset\n",
    "        X_num = X[self.numeric_cols_].copy()\n",
    "\n",
    "        # Escalona as colunas, exceto a coluna alvo\n",
    "        X_num_scaled = self.scaler_.transform(X_num.drop(columns=[self.target_col]))\n",
    "\n",
    "        # Realiza imputação concatenando as features com a coluna alvo (que pode ter NAs)\n",
    "        imputed = self.imputer_.transform(np.c_[X_num_scaled, X_num[self.target_col].values.reshape(-1,1)])\n",
    "\n",
    "        # Atualiza a coluna alvo com os valores imputados\n",
    "        X_num[self.target_col] = imputed[:, -1]\n",
    "\n",
    "        # Cria uma cópia do DataFrame original para não alterar inplace\n",
    "        X_new = X.copy()\n",
    "\n",
    "        # Atualiza as colunas numéricas com os valores (incluindo a coluna alvo imputada)\n",
    "        X_new[self.numeric_cols_] = X_num\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Combina os passos fit e transform para facilitar uso.\n",
    "\n",
    "        Parâmetros:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Dataset para ajustar e transformar.\n",
    "        y : Ignorado (compatibilidade sklearn)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        X_new : pd.DataFrame\n",
    "            Dataset com coluna alvo imputada após ajuste.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9ad38",
   "metadata": {},
   "source": [
    "Exemplo prático de aplicação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365bb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que a classe CombinedAttributeOther já foi definida/importada aqui\n",
    "\n",
    "# Criando um DataFrame de exemplo com valores faltantes na coluna alvo 'target'\n",
    "data = {\n",
    "    'feature1': [1.0, 2.0, 3.0, 4.0, 5.0, np.nan, 7.0],\n",
    "    'feature2': [10, 9, 8, 7, 6, 5, 4],\n",
    "    'target': [100, 200, np.nan, 400, 500, 600, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset original:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedfe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o imputador para a coluna alvo 'target'\n",
    "imputer = CombinedAttributeOther(target_col='target')\n",
    "\n",
    "# 1. Usando fit(): ajusta o imputador nos dados\n",
    "imputer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando transform(): aplica a imputação (necessário já ter feito fit)\n",
    "df_imputed = imputer.transform(df)\n",
    "\n",
    "print(\"\\nDataset após transform (imputação aplicada):\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o imputador para a coluna alvo 'target'\n",
    "imputer = CombinedAttributeOther(target_col='feature1')\n",
    "\n",
    "# 3. Usando fit_transform(): faz o ajuste e já retorna o dataset imputado\n",
    "df_imputed_2 = imputer.fit_transform(df)\n",
    "\n",
    "print(\"\\nDataset após fit_transform (ajuste + imputação):\")\n",
    "print(df_imputed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.best_k_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4258466",
   "metadata": {},
   "source": [
    "### ESCALONAMENTO DOS DADOS\n",
    "\n",
    "O escalonamento de dados é uma etapa crucial no pré-processamento, especialmente quando se utiliza algoritmos baseados em distância, como o KNN. O escalonamento garante que todas as variáveis contribuam igualmente para a distância calculada, evitando que variáveis com escalas maiores dominem a análise. Para além do KNN, o escalonamento é importante para muitos algoritmos de aprendizado de máquina, como regressão logística, SVM e redes neurais. O escalonamento pode ser feito de várias maneiras, incluindo:\n",
    "- **Min-Max Scaling:** Transforma os dados para um intervalo específico, geralmente [0, 1].\n",
    "- **Padronização (Z-score):** Transforma os dados para que tenham média 0 e desvio padrão 1.\n",
    "- **Robust Scaling:** Usa a mediana e o intervalo interquartil para escalonar os dados, tornando-o robusto a outliers.\n",
    "- **Log Transformation:** Aplica a transformação logarítmica para lidar com distribuições assimétricas.\n",
    "- **Quantile Transformation:** Transforma os dados para uma distribuição uniforme ou normal, útil para lidar com distribuições não gaussianas.\n",
    "- **E entre outros...**\n",
    "\n",
    "Entretanto, em Machine Learning, o escalonamento é uma etapa importante, mas não é sempre necessário. Por exemplo, algoritmos baseados em árvores (como Decision Trees e Random Forests) não são sensíveis à escala dos dados, então o escalonamento pode não ser necessário. No entanto, para algoritmos que dependem de distâncias, como KNN e SVM, o escalonamento é essencial para garantir um desempenho adequado. E mais, como em todas as etapas do pré-processamento, o escalonamento deve ser feito com cuidado, considerando o contexto dos dados e o algoritmo que será utilizado. O uso de técnicas de escalonamento pode melhorar a performance do modelo e garantir que todas as variáveis sejam tratadas de forma justa. Como em todas as transformações, é importante ajustar os escalonamentos apenas aos dados de treinamento, não ao conjunto de dados completo (incluindo o conjunto de testes). Só então, o escalonamento deve ser aplicado ao conjunto de teste usando os parâmetros calculados no conjunto de treinamento. Isso garante que o modelo seja avaliado de forma justa e evita vazamento de dados.\n",
    "\n",
    "### Demonstração Matemática dos Métodos de Escalonamento\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Padronização (StandardScaler)\n",
    "\n",
    "A padronização transforma os dados para que tenham média zero e desvio padrão 1, aplicando a fórmula:\n",
    "\n",
    "$\n",
    "x' = \\dfrac{x - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $x$ é o valor original da amostra,\n",
    "- $\\mu$ é a média dos dados do conjunto (geralmente calculada no conjunto de treinamento),\n",
    "- $\\sigma$ é o desvio padrão dos dados do conjunto.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. MinMaxScaler\n",
    "\n",
    "O MinMaxScaler reescala os dados para um intervalo específico $[a, b]$. A fórmula geral é:\n",
    "\n",
    "$\n",
    "x' = a + \\dfrac{(x - x_{\\min})(b - a)}{x_{\\max} - x_{\\min}}\n",
    "$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $x$ é o valor original da amostra,\n",
    "- $x_{\\min}$ e $x_{\\max}$ são, respectivamente, o valor mínimo e máximo dos dados do conjunto,\n",
    "- $a$ e $b$ são os limites inferior e superior do intervalo desejado.\n",
    "\n",
    "---\n",
    "\n",
    "##### Caso 1: MinMaxScaler para o intervalo $[0, 1]$\n",
    "\n",
    "Aqui, $a = 0$ e $b = 1$, então a fórmula simplifica para:\n",
    "\n",
    "$\n",
    "x' = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "##### Caso 2: MinMaxScaler para o intervalo $[1, 1]$\n",
    "\n",
    "Note que $[1, 1]$ é um intervalo degenerado, ou seja, os limites inferior e superior são iguais. Nesse caso, a fórmula se torna:\n",
    "\n",
    "$\n",
    "x' = 1 + \\dfrac{(x - x_{\\min})(1 - 1)}{x_{\\max} - x_{\\min}} = 1 + 0 = 1\n",
    "$\n",
    "\n",
    "Ou seja, **todos os valores são mapeados para 1**, já que não há variação no intervalo.\n",
    "\n",
    "---\n",
    "\n",
    "# Resumo\n",
    "\n",
    "| Técnica         | Fórmula                                               | Intervalo padrão         |\n",
    "|-----------------|------------------------------------------------------|-------------------------|\n",
    "| Padronização    | $x' = \\dfrac{x - \\mu}{\\sigma}$                      | Média 0, desvio padrão 1 |\n",
    "| MinMaxScaler    | $x' = a + \\dfrac{(x - x_{\\min})(b - a)}{x_{\\max} - x_{\\min}}$ | Variável, ex: [0, 1]    |\n",
    "| MinMaxScaler [0,1] | $x' = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$ | [0, 1]                  |\n",
    "| MinMaxScaler [1,1] | $x' = 1$                                          | Degenerado, tudo igual 1 |\n",
    "\n",
    "Exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff98a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎲 Define uma semente (seed) para o gerador de números aleatórios\n",
    "# Isso garante que os resultados sejam reproduzíveis, ou seja, \n",
    "# toda vez que rodar o código, os números gerados serão os mesmos\n",
    "np.random.seed(42)\n",
    "\n",
    "# 🎯 Cria um conjunto de dados aleatórios seguindo uma distribuição normal (Gaussiana)\n",
    "\n",
    "data = np.random.normal(\n",
    "    loc=20,      # 👉 loc é a MÉDIA da distribuição (nesse caso, 20)\n",
    "    scale=5,     # 👉 scale é o DESVIO PADRÃO (a dispersão dos dados, aqui é 5)\n",
    "    size=1000    # 👉 size define a quantidade de valores que serão gerados (1000 valores)\n",
    ").reshape(-1, 1) # 👉 reshape(-1, 1) transforma o array de uma dimensão (1D) \n",
    "                 # em um array bidimensional (2D) com 1000 linhas e 1 coluna.\n",
    "                 # Isso é útil para compatibilidade com modelos e funções \n",
    "                 # do scikit-learn, que geralmente trabalham com matrizes (2D).\n",
    "\n",
    "# 🔍 Resultado:\n",
    "# 'data' é um array de forma (1000, 1) contendo 1000 valores simulados\n",
    "# que seguem uma distribuição normal com média 20 e desvio padrão 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Padronização padrão com StandardScaler (média = 0, variância = 1)\n",
    "\n",
    "# 🔧 Cria um objeto do StandardScaler\n",
    "# Esse scaler transforma os dados para que tenham:\n",
    "# → média = 0\n",
    "# → desvio padrão = 1 (variância = 1)\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# 🚀 Ajusta o scaler aos dados (fit) e transforma (transform) de uma vez\n",
    "# → Calcula a média e o desvio padrão dos dados\n",
    "# → Retorna um array padronizado: (x - média) / desvio_padrão\n",
    "data_standard = scaler_standard.fit_transform(data)\n",
    "\n",
    "\n",
    "# 2️⃣ Padronização customizada: média = 5, variância = 2\n",
    "\n",
    "# 👉 Aqui pegamos os dados já padronizados (média = 0, desvio = 1)\n",
    "# e fazemos uma transformação linear para alterar a escala:\n",
    "\n",
    "# Multiplicamos pelos novo desvio padrão:\n",
    "# → Desvio padrão = √2, pois variância = 2 (variância = desvio²)\n",
    "# E somamos a nova média = 5\n",
    "data_standard_custom = data_standard * np.sqrt(2) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28936016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Cria um objeto MinMaxScaler com intervalo padrão (0, 1)\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# 🚀 Ajusta (fit) e transforma (transform) os dados\n",
    "# → Escala os dados para que todos fiquem no intervalo de 0 a 1\n",
    "data_minmax = scaler_minmax.fit_transform(data)\n",
    "\n",
    "# 🔧 Cria um MinMaxScaler que ajusta os dados para o intervalo (0, 10)\n",
    "scaler_minmax_custom = MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "# 🚀 Ajusta e transforma os dados para esse novo intervalo\n",
    "data_minmax_custom = scaler_minmax_custom.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria figura com 2 linhas x 2 colunas de subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 9))                    # figsize define o tamanho em polegadas\n",
    "fig.suptitle('Comparação dos Métodos de Escalonamento de Dados', fontsize=16, weight='bold')\n",
    "\n",
    "# Subplot 1: histograma da padronização padrão\n",
    "axs[0, 0].hist(data_standard, bins=40,                           # Número de barras no histograma\n",
    "               color='steelblue', edgecolor='black', alpha=0.8)  # Cores e transparência\n",
    "axs[0, 0].set_title('Padronização Padrão\\n(média=0, variância=1)', fontsize=12, weight='bold')\n",
    "axs[0, 0].set_xlabel('Valor padronizado')                        # Label do eixo X\n",
    "axs[0, 0].set_ylabel('Frequência')                               # Label do eixo Y\n",
    "axs[0, 0].grid(True)                                             # Ativa grade\n",
    "\n",
    "# Subplot 2: histograma da padronização customizada\n",
    "axs[0, 1].hist(data_standard_custom, bins=40,\n",
    "               color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "axs[0, 1].set_title('Padronização Customizada\\n(média=5, variância=2)', fontsize=12, weight='bold')\n",
    "axs[0, 1].set_xlabel('Valor padronizado customizado')\n",
    "axs[0, 1].set_ylabel('Frequência')\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Subplot 3: histograma do MinMaxScaler padrão (0 a 1)\n",
    "axs[1, 0].hist(data_minmax, bins=40,\n",
    "               color='tomato', edgecolor='black', alpha=0.8)\n",
    "axs[1, 0].set_title('MinMaxScaler Padrão\\n(Intervalo: 0 a 1)', fontsize=12, weight='bold')\n",
    "axs[1, 0].set_xlabel('Valor escalado (0–1)')\n",
    "axs[1, 0].set_ylabel('Frequência')\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# Subplot 4: histograma do MinMaxScaler customizado (0 a 10)\n",
    "axs[1, 1].hist(data_minmax_custom, bins=40,\n",
    "               color='darkorange', edgecolor='black', alpha=0.8)\n",
    "axs[1, 1].set_title('MinMaxScaler Customizado\\n(Intervalo: 0 a 10)', fontsize=12, weight='bold')\n",
    "axs[1, 1].set_xlabel('Valor escalado (0–10)')\n",
    "axs[1, 1].set_ylabel('Frequência')\n",
    "axs[1, 1].grid(True)\n",
    "\n",
    "# Ajusta layout para evitar sobreposição de elementos\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df244449",
   "metadata": {},
   "source": [
    "## **Feature Engeneering**\n",
    "\n",
    "Como diz o ditado: entra lixo, sai lixo. Seu sistema de só será capaz de aprender se os dados de treinamento tiverem características relevantes o suficientes e poucas características irrelevantes. Uma parte imprescindível do sucesso de um projeto de ML é criar um bom conjunto de características para o treinamento, processo chamado de *feature engeneering* (ou engenharia de features) que envolve os seguintes passos:\n",
    "\n",
    "- *Seleção de características* (selecionar as características mais úteis para treinamento entre as características existentes)\n",
    "- *Extração de características* (combinar características existentes a fim de obter as mais úteis)\n",
    "- Criação de novas características ao coletar dados novos.\n",
    "\n",
    "Até o momento, lidamos apenas com atributos numéricos, mas agora analisaremos os atributosde texto. Neste conjunto de dados, existe somente uma variável: o atributo ``ocean_proximity``, vamos analisar ele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Calcula as porcentagens de cada categoria na coluna 'ocean_proximity'\n",
    "ocean_proximity_percent = round((housing_imputed.ocean_proximity.value_counts() / len(housing_imputed)) * 100, 2)\n",
    "\n",
    "# Cria dataframe\n",
    "df_percent = pd.DataFrame({\n",
    "    'Localização': ocean_proximity_percent.index,\n",
    "    'Porcentagem (%)': ocean_proximity_percent.values\n",
    "})\n",
    "\n",
    "# 📜 Gera a string em formato Markdown para exibir como tabela\n",
    "markdown_table = \"### Distribuição das Localizações (`ocean_proximity`)\\n\\n\"\n",
    "markdown_table += df_percent.to_markdown(index=False)\n",
    "\n",
    "# 🔥 Exibe como markdown\n",
    "display(Markdown(markdown_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45c526",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02373efd",
   "metadata": {},
   "source": [
    "A maioria dos algoritmos de ML prefere trabalhar com números, para corrigir esse problema, uma solução comum é criar um atributo binário por categoria, isso se chama *codificação one-hot* [*one-hot enconding* ou ainda *codificação distribuída*], porque apenas um atributo será igual a 1, enquanto os outros serão 0. Os atributos novos se chamam atributos falsos [*dummy*]. ``sklearn`` fornece uma uma classe ``OneHotEncoder()`` para converter valores categóricos em valores one-hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e189c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_imputed[['ocean_proximity']])\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e0977",
   "metadata": {},
   "source": [
    "### Combinações de Atributos\n",
    "\n",
    "Uma das últimas que você pode querer fazer antes de preparar os dados para os algoritmos de ML é testar diferentes combinações de atributos para gerar novos atributos. Por exemplo:\n",
    "- O número total de cômodos em uma determinada região não servirá de nadda se não souber quantas famílias vivem nessa região.\n",
    "- Do mesmo modo, o número total de quartos propriamente dito não ajuda muito: você provavelmente vai querer comprá-lo com o número de cômodos.\n",
    "- E ao que tudo indica, a população por domicílio também é uma combinação de taributos interressante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f31de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Feature Engineering: criação de novas variáveis\n",
    "\n",
    "# 🔧 rooms_per_household -> número médio de quartos por domicílio\n",
    "housing_imputed['rooms_per_household'] = housing_imputed['total_rooms'] / housing_imputed['households']\n",
    "\n",
    "# 🔧 bedrooms_per_room -> proporção de quartos que são dormitórios (mede a densidade de dormitórios)\n",
    "housing_imputed['bedrooms_per_room'] = housing_imputed['total_bedrooms'] / housing_imputed['total_rooms']\n",
    "\n",
    "# 🔧 population_for_household -> número médio de pessoas por domicílio\n",
    "housing_imputed['population_for_household'] = housing_imputed['population'] / housing_imputed['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c868448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Seleciona apenas as novas variáveis criadas\n",
    "df_new_features = housing_imputed[['rooms_per_household', 'bedrooms_per_room', 'population_for_household']].head()\n",
    "\n",
    "# 📜 Cria a string da tabela em Markdown\n",
    "markdown_table = \"### 🔍 Novas Variáveis Criadas (`Feature Engineering`)\\n\\n\"\n",
    "markdown_table += df_new_features.to_markdown(index=False)\n",
    "\n",
    "# 📊 Exibe como Markdown no output\n",
    "display(Markdown(markdown_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd607711",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing_imputed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada56aa",
   "metadata": {},
   "source": [
    "### Seleção de Variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a99356",
   "metadata": {},
   "source": [
    "#### Visualizando Dados Geográficos\n",
    "\n",
    "Uma vez que temos informações geográficas (latitude e longutude), é uma boa ideia criar um diagrama de dispersão para visualizar os dados de todas as regiões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem de um gráfico de dispersão (scatter plot) usando DataFrame 'housing_imputed'\n",
    "# Esse gráfico representa as casas de acordo com sua longitude e latitude, com várias codificações visuais.\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Define o tamanho da figura em polegadas (mais espaço, melhor visualização)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    housing_imputed['longitude'],             # Eixo x: longitude das casas\n",
    "    housing_imputed['latitude'],              # Eixo y: latitude das casas\n",
    "    alpha=0.5,                                # Transparência dos pontos, de 0 (invisível) a 1 (opaco)\n",
    "    s=housing_imputed['population'] / 100,    # Tamanho dos pontos proporcional à população local\n",
    "    c=housing_imputed['median_house_value'],  # Cor dos pontos de acordo com o valor mediano das casas\n",
    "    cmap='viridis',                           # Mapa de cores mais moderno e perceptível (substituindo 'jet')\n",
    "    edgecolor='k',                            # Adiciona contorno preto aos pontos\n",
    "    linewidth=0.5                             # Define a espessura da linha do contorno\n",
    ")\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=14)          # Rótulo do eixo x com tamanho de fonte maior\n",
    "plt.ylabel('Latitude', fontsize=14)           # Rótulo do eixo y\n",
    "\n",
    "plt.title('Distribuição Geográfica das Casas na Califórnia', fontsize=16)  # Título do gráfico\n",
    "\n",
    "cbar = plt.colorbar(scatter)                  # Adiciona barra de cores ao lado\n",
    "cbar.set_label('Valor Mediano das Casas', fontsize=12)  # Rótulo da barra de cores\n",
    "\n",
    "plt.legend(['População (escala do tamanho dos pontos)'], fontsize=12)  # Legenda explicando o tamanho\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)     # Adiciona grade com linhas tracejadas e leve transparência\n",
    "\n",
    "plt.tight_layout()                            # Ajusta automaticamente o layout para não cortar elementos\n",
    "\n",
    "plt.show()                                    # Exibe o gráfico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9737c4",
   "metadata": {},
   "source": [
    "O raio de cada círculo representa a população (opção ``s``) e a cor representa o preço (opção ``c``). Usamos um mapa de cores predefinido (opção ``cmap``) chamado ``virids``, que varia de roxo (valores baixos) para amarelo (valores altos). Esta imagem informa que os preços dos imóveis estão muito relacionados à localização (por exemplo proximidade do mar) e à densidade populacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0fd1e",
   "metadata": {},
   "source": [
    "#### Buscando **Correlações**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f1e07",
   "metadata": {},
   "source": [
    "##### Correlação de Pearson\n",
    "\n",
    "A correlação de Pearson é frequentemente aplicada de forma direta em análises exploratórias, como visto no livro de Aurélien Géron (*Hands-On Machine Learning*), devido à sua simplicidade e utilidade para rapidamente identificar padrões de dependência linear entre variáveis.\n",
    "\n",
    "No entanto, é importante destacar que, para uma interpretação estatística rigorosa e inferencial da correlação, recomenda-se verificar os seguintes pressupostos: linearidade, normalidade, homocedasticidade, escala intervalar e ausência de outliers.\n",
    "\n",
    "1️⃣ O objetivo final é predição, não inferência causal.\n",
    "\n",
    "Logo, o foco não é fazer testes estatísticos para validar hipóteses ou explicar relações entre variáveis com validade inferencial, mas **melhorar a acurácia ou outra métrica do modelo preditivo**.\n",
    "\n",
    "➡️ Nestes casos, **NÃO é necessário exigir os pressupostos clássicos da correlação de Pearson**, como linearidade ou normalidade.\n",
    "\n",
    "➡️ A correlação pode ser usada como uma **heurística rápida** para identificar redundâncias ou dependências fortes, e guiar a escolha de features.\n",
    "\n",
    "##### 🧠 **Lógica da Correlação de Pearson**\n",
    "\n",
    "A correlação de Pearson mede o grau de **associação linear** entre duas variáveis numéricas.\n",
    "\n",
    "A fórmula é:\n",
    "\n",
    "$\n",
    "r = \\dfrac{\\sum (X_i - \\bar{X}) (Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum (Y_i - \\bar{Y})^2}}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $X_i$ e $Y_i$ são os valores das variáveis X e Y.\n",
    "- $\\bar{X}$ e $\\bar{Y}$ são as médias de X e Y.\n",
    "- $r$ é o coeficiente de correlação de Pearson, que varia entre -1 e 1.\n",
    "\n",
    "**Interpretação:**\n",
    "\n",
    "- $r = 1$: Correlação linear positiva perfeita.\n",
    "- $r = -1$: Correlação linear negativa perfeita.\n",
    "- $r = 0$: Ausência de correlação linear (mas não implica independência total).\n",
    "\n",
    "Ele é, basicamente, a **covariância padronizada** entre duas variáveis. A padronização ocorre ao dividir pela multiplicação dos desvios padrão, permitindo que o coeficiente sempre esteja na escala de -1 a 1.\n",
    "\n",
    "---\n",
    "\n",
    "##### Correlação de Spearman\n",
    "\n",
    "A correlação de Spearman é uma medida não-paramétrica que avalia a **força e a direção da associação monotônica** entre duas variáveis, isto é, verifica se à medida que uma variável aumenta, a outra tende a aumentar ou diminuir, sem exigir que essa relação seja linear.\n",
    "\n",
    "É especialmente útil quando:\n",
    "\n",
    "- Os dados não seguem uma distribuição normal.\n",
    "- Há presença de outliers.\n",
    "- As relações são não-lineares, mas ainda monotônicas.\n",
    "\n",
    "##### 🧠 **Lógica da Correlação de Spearman**\n",
    "\n",
    "O primeiro passo é transformar os dados em **ranks** (ordens). Cada valor de X e Y é substituído por sua posição na ordenação dos dados.\n",
    "\n",
    "A fórmula da correlação de Spearman, quando não há empates, é:\n",
    "\n",
    "$\n",
    "r_s = 1 - \\dfrac{6 \\sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $d_i$ = diferença entre os ranks de cada par de observações $(X_i, Y_i)$.\n",
    "- $n$ = número de observações.\n",
    "- $r_s$ = coeficiente de correlação de Spearman.\n",
    "\n",
    "Se houver empates nos dados, usa-se a mesma lógica da correlação de Pearson, mas aplicada aos ranks em vez dos valores originais.\n",
    "\n",
    "**Interpretação:**\n",
    "\n",
    "- $r_s = 1$: Relação monotônica crescente perfeita.\n",
    "- $r_s = -1$: Relação monotônica decrescente perfeita.\n",
    "- $r_s = 0$: Ausência de relação monotônica.\n",
    "\n",
    "➡️ Assim como a correlação de Pearson mede associações lineares, a de Spearman amplia essa análise para relações monotônicas, oferecendo maior robustez em cenários mais realistas do mundo dos dados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13574360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from correlation_analyzer import CorrelationAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00428b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = CorrelationAnalyzer(housing_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.correlation(target_col='median_house_value', method='pearson', sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.correlation(target_col='median_house_value', method='spearman', sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4825203",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot_correlation_matrix(figsize= (10, 10), method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot_correlation_matrix(figsize= (10, 10), method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7046525",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot_scatter_matrix(figsize=(12, 12), variables=['median_house_value', 'median_income', 'total_rooms',\n",
    "                                                    'housing_median_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa04667",
   "metadata": {},
   "source": [
    "## **Criando o Conjunto de Treinamento e Teste**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7db1a",
   "metadata": {},
   "source": [
    "Pode parecer estranho separar voluntariamente uma parte dos dados neste estágio. Afinal, você apenas deu uma olhada rápida nos dados, e certamente deveria aprender muito mais sobre eles antes de decidir quais algoritmos utilizar, certo? Isso é verdade, mas seu cérebro é um sistema incrível de detecção de padrões, o que também significa que ele é altamente propenso ao **overfitting**: se você olhar para o conjunto de teste, pode acabar encontrando algum padrão aparentemente interessante nos dados de teste que o leva a escolher um tipo específico de modelo de machine learning.\n",
    "\n",
    "Quando você estima o erro de generalização usando o conjunto de teste, essa estimativa será **otimista demais**, e você pode acabar lançando um sistema que não terá um desempenho tão bom quanto o esperado. Isso é conhecido como **data snooping bias** (viés de bisbilhotagem dos dados).\n",
    "\n",
    "A criação de um conjunto de teste é, teoricamente, simples: selecione algumas instâncias aleatoriamente — tipicamente **20% do conjunto de dados** (ou menos, se seu dataset for muito grande) — e separe-as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    \"\"\"\n",
    "    Divide os dados em conjuntos de treinamento e teste de forma aleatória.\n",
    "\n",
    "    -------------------------------\n",
    "    🔧 Parâmetros:\n",
    "    - data: DataFrame contendo os dados que serão divididos.\n",
    "      ➡️ É o dataset completo que você deseja particionar.\n",
    "\n",
    "    - test_ratio: float\n",
    "      ➡️ Proporção dos dados que serão separados para o conjunto de teste.\n",
    "      ➡️ Exemplo: se test_ratio = 0.2, então 20% dos dados serão usados para teste,\n",
    "         e 80% para treino.\n",
    "\n",
    "    -------------------------------\n",
    "    🔙 Retorna:\n",
    "    - train_data: DataFrame com os dados de treinamento.\n",
    "    - test_data: DataFrame com os dados de teste.\n",
    "\n",
    "    -------------------------------\n",
    "    🧠 Lógica do algoritmo:\n",
    "\n",
    "    1️⃣ Cria uma sequência de índices embaralhados dos dados.\n",
    "    2️⃣ Calcula o tamanho do conjunto de teste com base no test_ratio.\n",
    "    3️⃣ Separa os índices do conjunto de teste e do conjunto de treino.\n",
    "    4️⃣ Retorna os subconjuntos correspondentes de treino e teste.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gera uma permutação aleatória dos índices do DataFrame.\n",
    "    # Isso garante que a divisão seja aleatória a cada execução.\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "\n",
    "    # Calcula o número de amostras que irão compor o conjunto de teste.\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "\n",
    "    # Seleciona os primeiros 'test_set_size' índices para o conjunto de teste.\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "\n",
    "    # O restante dos índices será usado para o conjunto de treinamento.\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "\n",
    "    # Retorna os subconjuntos de treino e teste, utilizando os índices gerados.\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67100a64",
   "metadata": {},
   "source": [
    "Podemos usar uma função como essa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aa1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing_imputed, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "### 🔍 Conjunto de Treinamento (`train_set`)\n",
    "\n",
    "{train_set.head().to_markdown(index=False)}\n",
    "\n",
    "### 🔍 Conjunto de Teste (`test_set`)\n",
    "\n",
    "{test_set.head().to_markdown(index=False)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a44d71",
   "metadata": {},
   "source": [
    "Isso funciona, mas há um defeito: ao executar novamente, será gerado um conjunto diferente de teste e treino! Ao longo do tempo, você veria todo o conjunto de dados ao repetir esse processo, entretanto o **Scikit-Learn** oferece algumas funções para dividir conjuntos de dados em múltiplos subconjuntos de diferentes formas. \n",
    "\n",
    "A função mais simples é a **`train_test_split()`**, que realiza praticamente a mesma operação que a função **`split_train_test()`** que definimos anteriormente, porém com alguns recursos adicionais importantes:\n",
    "\n",
    "1️⃣ Primeiramente, há o parâmetro **`random_state`**, que permite definir a semente do gerador aleatório. Isso garante que a divisão dos dados seja **reprodutível**, ou seja, sempre gere os mesmos resultados se a mesma semente for usada.\n",
    "\n",
    "2️⃣ Além disso, é possível passar **múltiplos conjuntos de dados com o mesmo número de linhas**, e a função irá dividi-los utilizando os **mesmos índices**. \n",
    "\n",
    "➡️ Isso é extremamente útil, por exemplo, quando você possui um DataFrame separado contendo as variáveis preditoras e outro com os rótulos (labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing_imputed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "### 🔍 Conjunto de Treinamento (`train_set`)\n",
    "\n",
    "{train_set.head().to_markdown(index=False)}\n",
    "\n",
    "### 🔍 Conjunto de Teste (`test_set`)\n",
    "\n",
    "{test_set.head().to_markdown(index=False)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c31411",
   "metadata": {},
   "source": [
    "Até agora, consideramos métodos de amostragem puramente aleatórios. Isso geralmente funciona bem se seu conjunto de dados for suficientemente grande (especialmente em relação ao número de atributos), mas caso contrário, há o risco de introduzir um **viés amostral significativo**.\n",
    "\n",
    "Por exemplo, quando os funcionários de uma empresa de pesquisas decidem ligar para 1.000 pessoas para fazer algumas perguntas, eles não escolhem essas 1.000 pessoas aleatoriamente de uma lista telefônica. Eles tentam garantir que essas pessoas sejam **representativas da população como um todo**, levando em conta as variáveis relevantes para a pesquisa.\n",
    "\n",
    "➡️ Por exemplo, a população dos Estados Unidos é composta por **51,1% de mulheres e 48,9% de homens**. Portanto, uma pesquisa bem conduzida nos EUA tentaria manter essa proporção na amostra: **511 mulheres e 489 homens**, pelo menos se houver a possibilidade de que as respostas variem entre os gêneros.\n",
    "\n",
    "Esse método é chamado de **amostragem estratificada** (**stratified sampling**): a população é dividida em subgrupos homogêneos, chamados de **estratos** (*strata*), e o número adequado de instâncias é selecionado de cada estrato para garantir que o conjunto de teste seja representativo da população como um todo.\n",
    "\n",
    "⚠️ Se as pessoas responsáveis pela pesquisa utilizassem apenas uma amostragem aleatória simples, haveria cerca de **10,7% de chance** de obter um conjunto de teste distorcido, com **menos de 48,5% de mulheres ou mais de 53,5% de mulheres**. De qualquer forma, os resultados da pesquisa provavelmente seriam **bastante enviesados**.\n",
    "\n",
    "---\n",
    "\n",
    "Agora, suponha que você tenha conversado com alguns especialistas que te informaram que a **renda mediana** (*median income*) é um atributo muito importante para prever o preço mediano das casas.\n",
    "\n",
    "📊 Você pode querer garantir que o conjunto de teste seja representativo das diferentes faixas de renda presentes no dataset.\n",
    "\n",
    "Porém, como a renda mediana é um atributo **contínuo e numérico**, é necessário primeiro transformá-lo em uma variável categórica, criando **faixas de renda**.\n",
    "\n",
    "🔍 Observando o histograma da renda mediana (como na Figura 2-8 do livro), percebe-se que a maioria dos valores está concentrada entre **1,5 e 6** (isto é, entre **US$ 15.000 e US$ 60.000**), mas algumas rendas vão muito além de 6.\n",
    "\n",
    "É importante garantir que haja uma quantidade suficiente de instâncias em cada estrato. Caso contrário, a estimativa da importância de determinado estrato poderá ser enviesada.\n",
    "\n",
    "⚠️ Isso significa que:\n",
    "- Não se deve criar estratos demais.\n",
    "- Cada estrato precisa ser grande o suficiente para gerar estatísticas confiáveis.\n",
    "\n",
    "---\n",
    "\n",
    "➡️ O código a seguir utiliza a função **`pd.cut()`** para criar um atributo de categoria de renda, com **cinco categorias** (rotuladas de 1 a 5):\n",
    "\n",
    "- Categoria 1: de **0 até 1.5** (ou seja, menos de **US$ 15.000**)\n",
    "- Categoria 2: de **1.5 até 3**\n",
    "- Categoria 3: de **3 até 4.5**\n",
    "- Categoria 4: de **4.5 até 6**\n",
    "- Categoria 5: de **6 em diante** (rendas muito altas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f35215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Cria uma nova coluna chamada 'income_cat' no DataFrame 'housing'.\n",
    "# Essa coluna representa categorias de renda, que são derivadas da variável 'median_income'.\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],  # 🔸 Variável numérica que será transformada em categorias.\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],  # 🔸 Define os intervalos (faixas) das categorias.\n",
    "    # ▪️ Intervalo 1: de 0 até 1.5\n",
    "    # ▪️ Intervalo 2: de 1.5 até 3.0\n",
    "    # ▪️ Intervalo 3: de 3.0 até 4.5\n",
    "    # ▪️ Intervalo 4: de 4.5 até 6.0\n",
    "    # ▪️ Intervalo 5: de 6.0 até infinito (rendas muito altas)\n",
    "    labels=[1, 2, 3, 4, 5]  # 🔸 Rótulos atribuídos a cada categoria.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8819c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    housing[\"income_cat\"].value_counts()  # Conta quantas ocorrências há em cada categoria de renda.\n",
    "    .sort_index()                         # Organiza na ordem dos índices das categorias (de 1 a 5).\n",
    "    .plot.bar(                            # Cria um gráfico de barras.\n",
    "        rot=0,                            # Mantém os rótulos do eixo X na horizontal (sem rotação).\n",
    "        grid=True                          # Adiciona uma grade no fundo do gráfico.\n",
    "    )\n",
    ")\n",
    "\n",
    "# 🔧 Configurações dos rótulos e título do gráfico.\n",
    "plt.xlabel(\"Categoria de renda\")           # Define o rótulo do eixo X.\n",
    "plt.ylabel(\"Número de distritos\")          # Define o rótulo do eixo Y.\n",
    "plt.title(\"Distribuição das categorias de renda\")  # Adiciona um título ao gráfico.\n",
    "\n",
    "# 🔥 Exibe o gráfico.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1712570",
   "metadata": {},
   "source": [
    "Agora estamos prontos para realizar uma **amostragem estratificada** baseada na categoria de renda. O Scikit-Learn oferece ferramentas específicas para isso. Podemos utilizar o ``StratifiedShuffleSplit`` do Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a classe StratifiedShuffleSplit do módulo model_selection do sklearn\n",
    "# Esta classe realiza divisões estratificadas dos dados preservando a proporção das classes\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Cria uma instância do StratifiedShuffleSplit com os seguintes parâmetros:\n",
    "# - n_splits=10: gera 1 divisão dos dados\n",
    "# - test_size=0.2: 20% dos dados serão usados para teste em cada divisão\n",
    "# - random_state=42: semente aleatória para reprodutibilidade\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop sobre cada divisão gerada pelo splitter:\n",
    "# - housing: DataFrame completo com os dados\n",
    "# - housing[\"income_cat\"]: coluna usada para estratificação (garante proporção igual em treino/teste)\n",
    "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
    "    # Cria conjunto de treino usando os índices gerados\n",
    "    strat_train_set = housing.iloc[train_index]\n",
    "    \n",
    "    # Cria conjunto de teste usando os índices gerados\n",
    "    strat_test_set = housing.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ef94c",
   "metadata": {},
   "source": [
    "Podemos ver as proporções da categoria de renda no conjunto de testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2591c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1606cf",
   "metadata": {},
   "source": [
    "### Análise Comparativa de Amostragem Estratificada vs Aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    \"\"\"Calcula as proporções das categorias de renda em um conjunto de dados.\n",
    "    \n",
    "    Parâmetros:\n",
    "        data (DataFrame): Conjunto de dados contendo a coluna 'income_cat'\n",
    "        \n",
    "    Retorna:\n",
    "        Series: Proporções de cada categoria de renda\n",
    "    \"\"\"\n",
    "    return data[\"income_cat\"].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide os dados de forma aleatória (não estratificada)\n",
    "# test_size=0.2 → 20% para teste, 80% para treino\n",
    "# random_state=42 → garante reprodutibilidade\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cria DataFrame comparativo das proporções:\n",
    "# - Overall: Proporções no dataset completo\n",
    "# - Stratified: Proporções no teste estratificado\n",
    "# - Random: Proporções no teste aleatório\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "\n",
    "# Calcula os erros percentuais das amostragens:\n",
    "# - Rand. %error: Diferença percentual da amostragem aleatória\n",
    "# - Strat. %error: Diferença percentual da amostragem estratificada\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "# Exibe a tabela comparativa\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea007b9",
   "metadata": {},
   "source": [
    "É possível medir as proporções das categorias de renda no conjunto de dados completo. A figura acima compara essas proporções em três cenários: (1) no dataset original, (2) no conjunto de teste gerado por amostragem estratificada e (3) no conjunto de teste criado com divisão puramente aleatória. Os resultados mostram que a **amostragem estratificada** preserva proporções quase idênticas às do dataset original, enquanto a divisão aleatória apresenta distorções significativas (*skew*). Isso demonstra a superioridade da estratificação para manter a representatividade estatística, especialmente em análises onde o balanceamento das categorias é crítico.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3a5e1",
   "metadata": {},
   "source": [
    "## Atividade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1d4b1",
   "metadata": {},
   "source": [
    "**Exercício de Pré-processamento**  \n",
    "Você irá:  \n",
    "Baixar o dataset Iris diretamente do repositório online usando `fetch_openml`:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Carregando dataset online (formato TGZ implícito)\n",
    "iris = fetch_openml('iris', version=1, as_frame=True)\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "```\n",
    "**Questões:**\n",
    "1. Aplicar **escalonamento** (à escolha: padronização ou normalização)  \n",
    "2. Dividir em treino/teste com **dois métodos** (aleatório e estratificado)  \n",
    "3. Verifique as proporções das classes em cada conjunto  \n",
    "\n",
    "**Dica:** Compare os resultados dos dois métodos de divisão para entender o impacto da estratificação em dados balanceados. O relatório final deve mostrar as métricas de proporção antes/depois do processamento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
