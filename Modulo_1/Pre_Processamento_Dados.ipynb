{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be7cffd",
   "metadata": {},
   "source": [
    "# Pr√©-Processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9534e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bem vindo ao Machine Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Lista de pacotes necess√°rios e os nomes para importar\n",
    "pacotes = {\n",
    "    \"pandas\": \"pd\",\n",
    "    \"numpy\": \"np\",\n",
    "    \"seaborn\": \"sns\",\n",
    "    \"matplotlib\": \"plt\",\n",
    "    \"IPython\": None,\n",
    "    \"scikit-learn\": None,\n",
    "    \"tabulate\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e335df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para instalar pacotes\n",
    "def instalar(pacote):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pacote])\n",
    "\n",
    "# Verifica e instala os pacotes\n",
    "for pacote, alias in pacotes.items():\n",
    "    try:\n",
    "        importlib.import_module(pacote)\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {pacote}...\")\n",
    "        instalar(pacote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d28f1",
   "metadata": {},
   "source": [
    "## Bibliotecas usadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b54323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a biblioteca pandas, amplamente utilizada para an√°lise e manipula√ß√£o de dados em Python.\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# O m√≥dulo 'os' fornece fun√ß√µes para interagir com o sistema operacional,\n",
    "# como manipula√ß√£o de arquivos, diret√≥rios e vari√°veis de ambiente.\n",
    "import os\n",
    "\n",
    "# O m√≥dulo 'tarfile' permite ler e escrever arquivos compactados no formato .tar, .tar.gz, .tgz, etc.\n",
    "# √â √∫til para extrair ou criar arquivos compactados.\n",
    "import tarfile\n",
    "\n",
    "# O m√≥dulo 'urllib' oferece fun√ß√µes para manipular URLs e fazer requisi√ß√µes HTTP,\n",
    "# como baixar arquivos da internet.\n",
    "import urllib\n",
    "\n",
    "# Importa a biblioteca Seaborn, que √© utilizada para cria√ß√£o de gr√°ficos estat√≠sticos,\n",
    "# tornando a visualiza√ß√£o de dados mais simples e visualmente agrad√°vel.\n",
    "import seaborn as sns\n",
    "\n",
    "# Importa o m√≥dulo pyplot da biblioteca Matplotlib, que fornece fun√ß√µes para gerar gr√°ficos\n",
    "# como linha, dispers√£o, barras, histogramas, entre outros, e permite controle total sobre eles.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Importando a biblioteca necess√°ria\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import necess√°rio\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Normalizar os dados antes de aplicar KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbbb84",
   "metadata": {},
   "source": [
    "## Obten√ß√£o dos Dados\n",
    "\n",
    "Pacotes usados:\n",
    "\n",
    "```python\n",
    "# Importa a biblioteca pandas, amplamente utilizada para an√°lise e manipula√ß√£o de dados em Python.\n",
    "import pandas as pd\n",
    "\n",
    "# O m√≥dulo 'os' fornece fun√ß√µes para interagir com o sistema operacional,\n",
    "# como manipula√ß√£o de arquivos, diret√≥rios e vari√°veis de ambiente.\n",
    "import os\n",
    "\n",
    "# O m√≥dulo 'tarfile' permite ler e escrever arquivos compactados no formato .tar, .tar.gz, .tgz, etc.\n",
    "# √â √∫til para extrair ou criar arquivos compactados.\n",
    "import tarfile\n",
    "\n",
    "# O m√≥dulo 'urllib' oferece fun√ß√µes para manipular URLs e fazer requisi√ß√µes HTTP,\n",
    "# como baixar arquivos da internet.\n",
    "import urllib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5d66f",
   "metadata": {},
   "source": [
    "Para este fase da oficina trabalharemos com um conjunto de dados dos im√≥veis em distritos da Calif√≥rnia, considerando uma s√©rie de caracter√≠sticas desses distritos, cada inst√¢ncia √© um distrito. Basta fazer o download do arquivo de um arquivo compactado, *housing.tgz*, que cont√©m o arquivo *housing.csv*.\n",
    "\n",
    "Voc√™ poderia usar o navegador para baixar o arquivo, mas √© prefer√≠vel criar uma pequena fun√ß√£o para tal. Ter uma fun√ß√£o para baixar arquivos √© uma boa pr√°tica, pois voc√™ pode reutiliz√°-la em outros projetos. Al√©m disso, voc√™ pode adicionar funcionalidades extras, como verificar se o arquivo j√° foi baixado ou n√£o.\n",
    "\n",
    "As fun√ß√µes abaixo fazem o download do arquivo, descompactam o arquivo e carregam os dados em um DataFrame do Pandas. O arquivo CSV cont√©m informa√ß√µes sobre os pre√ßos de im√≥veis na Calif√≥rnia, incluindo caracter√≠sticas como n√∫mero de quartos, idade da casa, localiza√ß√£o e outros fatores que podem influenciar o pre√ßo. Esses dados s√£o frequentemente usados em an√°lises de pre√ßos de im√≥veis e modelos preditivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c021af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml/master/'\n",
    "# Define a URL base de onde os dados ser√£o baixados.\n",
    "\n",
    "HOUSING_PATH = os.path.join('datasets', 'housing')\n",
    "# Cria um caminho local ('datasets/housing') para armazenar os dados baixados.\n",
    "# Usa os.path.join para garantir compatibilidade entre sistemas operacionais.\n",
    "\n",
    "HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'\n",
    "# Monta a URL completa do arquivo compactado que ser√° baixado,\n",
    "# juntando a URL base com o caminho do arquivo no reposit√≥rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o respons√°vel por:\n",
    "    1. Criar uma pasta local para armazenar os dados (se ela n√£o existir).\n",
    "    2. Baixar um arquivo compactado (.tgz) de um reposit√≥rio online.\n",
    "    3. Extrair esse arquivo para a pasta especificada.\n",
    "\n",
    "    Par√¢metros:\n",
    "    - housing_url: URL onde est√° localizado o arquivo compactado com os dados.\n",
    "    - housing_path: Caminho local onde os dados ser√£o salvos e extra√≠dos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cria o diret√≥rio onde os dados ser√£o armazenados.\n",
    "    # Se o diret√≥rio j√° existir, 'exist_ok=True' evita que um erro seja gerado.\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "\n",
    "    # Define o caminho completo onde o arquivo .tgz ser√° salvo localmente.\n",
    "    # Junta o caminho da pasta com o nome do arquivo.\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
    "\n",
    "    # Faz o download do arquivo a partir da URL especificada.\n",
    "    # Salva o arquivo compactado no caminho definido por 'tgz_path'.\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "\n",
    "    # Abre o arquivo compactado (.tgz) para leitura.\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "\n",
    "    # Extrai todo o conte√∫do do arquivo compactado para a pasta definida.\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "\n",
    "    # Fecha o arquivo .tgz para liberar recursos do sistema.\n",
    "    housing_tgz.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e790a",
   "metadata": {},
   "source": [
    "Quando chamamos `fetch_housing_data()` cria um diret√≥rio *datasets/housing* em seu workspace, baixa o arquivo *housing.tgz* e extrai o arquivo *housing.csv* nesse diret√≥rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd4361",
   "metadata": {},
   "source": [
    "Quando chamamos `fetch_housing_data()`, ele cria um diret√≥rio chamado `datasets/housings` em seu workspace, faz o download do arquivo *housing.tgz* e extrai o arquivo *housing.csv* para esse diret√≥rio. Agora carregaremos carregaremos os dados com o Pandas. Mais uma vez, voc√™ deve descrever uma pequena fun√ß√£o para carreg√°-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o respons√°vel por carregar os dados de habita√ß√£o (housing) que est√£o armazenados em um arquivo CSV.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - housing_path: Caminho da pasta onde o arquivo 'housing.csv' est√° localizado.\n",
    "    \n",
    "    Retorna:\n",
    "    - Um DataFrame do pandas contendo os dados carregados do arquivo CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define o caminho completo do arquivo CSV, unindo o caminho da pasta com o nome do arquivo.\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "\n",
    "    # Usa a fun√ß√£o read_csv do pandas para ler o arquivo CSV localizado em 'csv_path'.\n",
    "    # Retorna o conte√∫do como um DataFrame, que √© uma estrutura de dados tabular (tabelas) muito poderosa no pandas.\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2d6c5",
   "metadata": {},
   "source": [
    "Vamos olhar as 5 primeiras linhas do DataFrame que iremos carregar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(housing.columns):\n",
    "    print(f'Vari√°vel {i+1}: {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107e5f9",
   "metadata": {},
   "source": [
    "---\n",
    "**Descri√ß√£o das Vari√°veis do Dataset Housing:**\n",
    "\n",
    "- **Vari√°vel 1:** `longitude` ‚Äî Longitude da localiza√ß√£o.\n",
    "- **Vari√°vel 2:** `latitude` ‚Äî Latitude da localiza√ß√£o.\n",
    "- **Vari√°vel 3:** `housing_median_age` ‚Äî Mediana da idade das constru√ß√µes residenciais naquela √°rea.\n",
    "- **Vari√°vel 4:** `total_rooms` ‚Äî N√∫mero total de c√¥modos (rooms) nas resid√™ncias da √°rea.\n",
    "- **Vari√°vel 5:** `total_bedrooms` ‚Äî N√∫mero total de quartos nas resid√™ncias da √°rea.\n",
    "- **Vari√°vel 6:** `population` ‚Äî Popula√ß√£o da √°rea.\n",
    "- **Vari√°vel 7:** `households` ‚Äî N√∫mero de domic√≠lios (households) na √°rea.\n",
    "- **Vari√°vel 8:** `median_income` ‚Äî Renda mediana dos moradores da √°rea (em dezenas de milhares de d√≥lares).\n",
    "- **Vari√°vel 9:** `median_house_value` ‚Äî Valor mediano das resid√™ncias naquela √°rea (em d√≥lares).\n",
    "- **Vari√°vel 10:** `ocean_proximity` ‚Äî Proximidade com o oceano (categorias como \"INLAND\", \"NEAR OCEAN\", etc.).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa394f89",
   "metadata": {},
   "source": [
    "## Pr√©-An√°lise Explorat√≥ria dos Dados\n",
    "\n",
    "Pacotes usados:\n",
    "\n",
    "```python\n",
    "# Importa a biblioteca Seaborn, que √© utilizada para cria√ß√£o de gr√°ficos estat√≠sticos,\n",
    "# tornando a visualiza√ß√£o de dados mais simples e visualmente agrad√°vel.\n",
    "import seaborn as sns\n",
    "\n",
    "# Importa o m√≥dulo pyplot da biblioteca Matplotlib, que fornece fun√ß√µes para gerar gr√°ficos\n",
    "# como linha, dispers√£o, barras, histogramas, entre outros, e permite controle total sobre eles.\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527413d",
   "metadata": {},
   "source": [
    "### Agora o momento de investiga√ß√£o dos dados üîç- *MUITA ATEN√á√ÉO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f744e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c65fd",
   "metadata": {},
   "source": [
    "O ``info()`` √© bom, entretanto podemos ter uma an√°lise mais minuciosa quanto ao tipo e quantidade de inst√¢ncias, vamos criar uma fun√ß√£o para isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o da classe DataAnalyzer\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Inicializa o analisador com um DataFrame.\n",
    "        \n",
    "        Par√¢metros:\n",
    "        - df: pandas DataFrame que ser√° utilizado para an√°lise, logo ele j√° tem de estar importado no ambiente.\n",
    "        \n",
    "        Atributos criados:\n",
    "        - self.df: guarda o DataFrame passado na cria√ß√£o do objeto.\n",
    "        - self.total: guarda a quantidade total de observa√ß√µes (linhas) do DataFrame.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.total = len(df)\n",
    "\n",
    "    def tipos_variaveis(self):\n",
    "        \"\"\"\n",
    "        Mostra a contagem dos tipos de vari√°veis presentes no DataFrame.\n",
    "        \n",
    "        Funcionalidade:\n",
    "        - Conta quantas vari√°veis s√£o de cada tipo (ex.: int64, float64, object, etc.).\n",
    "        - Exibe os resultados formatados visualmente, com separadores e emojis.\n",
    "        \"\"\"\n",
    "        print('='*50)  # Linha de separa√ß√£o\n",
    "        print('üìä Tipos de Vari√°veis:')  # T√≠tulo da se√ß√£o\n",
    "        print('-'*50)  # Linha de separa√ß√£o\n",
    "        print(self.df.dtypes.value_counts())  # Conta e exibe os tipos de dados das colunas\n",
    "        print('='*50)  # Linha de fechamento\n",
    "\n",
    "    def analise_dados_nulos(self):\n",
    "        \"\"\"\n",
    "        Realiza uma an√°lise de dados nulos no DataFrame.\n",
    "        \n",
    "        Funcionalidade:\n",
    "        - Mostra para cada coluna:\n",
    "            - Quantidade de valores n√£o nulos.\n",
    "            - Quantidade de valores nulos.\n",
    "            - Porcentagem de valores n√£o nulos.\n",
    "            - Porcentagem de valores nulos.\n",
    "        - Organiza essas informa√ß√µes em formato tabular, bem alinhado.\n",
    "        \"\"\"\n",
    "        print('='*50)\n",
    "        print('üîç An√°lise de Dados Nulos:')\n",
    "        print('-'*50)\n",
    "\n",
    "        # Cria um DataFrame auxiliar com as informa√ß√µes de nulos e n√£o nulos\n",
    "        dados_nulos = pd.DataFrame({\n",
    "            'N√£o Nulos': self.df.notnull().sum(),  # Quantidade de n√£o nulos por coluna\n",
    "            'Nulos': self.df.isnull().sum(),  # Quantidade de nulos por coluna\n",
    "            'Porcentagem N√£o Nulos (%)': (self.df.notnull().sum() / self.total * 100).round(2),  # Porcentagem de n√£o nulos\n",
    "            'Porcentagem Nulos (%)': (self.df.isnull().sum() / self.total * 100).round(2)  # Porcentagem de nulos\n",
    "        })\n",
    "\n",
    "        # Exibe o DataFrame de dados nulos em formato string, alinhado como tabela\n",
    "        print(dados_nulos.to_string())  # Essa linha faz com que a tabela fique alinhada horizontalmente, sem quebra\n",
    "        print('='*50)  # Linha de fechamento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eabc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing).tipos_variaveis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85b0a5",
   "metadata": {},
   "source": [
    "Vamos enxergar melhor as NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a209dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma nova figura para o gr√°fico, definindo o tamanho (largura=10, altura=5).\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Gera um heatmap (mapa de calor) utilizando seaborn.\n",
    "# O DataFrame housing.isnull() retorna True para valores nulos e False para n√£o nulos.\n",
    "# Cada quadrado do heatmap representa se h√° (ou n√£o) um valor nulo na respectiva c√©lula.\n",
    "# cbar=False remove a barra de cores lateral (opcional).\n",
    "# cmap='viridis' define o esquema de cores (pode ser 'viridis', 'magma', 'coolwarm', etc.).\n",
    "# yticklabels=False remove os r√≥tulos dos √≠ndices no eixo Y (para deixar o gr√°fico mais limpo).\n",
    "sns.heatmap(housing.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "\n",
    "# Adiciona um t√≠tulo ao gr√°fico.\n",
    "plt.title('Heatmap de valores Nulos (NA) no DataFrame')\n",
    "\n",
    "# Define o r√≥tulo (label) do eixo X, que representa as colunas do DataFrame.\n",
    "plt.xlabel('Colunas')\n",
    "\n",
    "# Rotaciona os nomes das colunas no eixo X para 45 graus,\n",
    "# facilitando a leitura quando os nomes s√£o longos ou numerosos.\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf3b8a",
   "metadata": {},
   "source": [
    "A maioria dos algoritmos de aprendizado de m√°quina n√£o lidam bem com dados ausentes. Portanto, √© importante identificar e tratar esses dados antes de prosseguir com a an√°lise ou modelagem. O tratamento de dados ausentes pode incluir a remo√ß√£o de linhas ou colunas que cont√™m valores ausentes, ou o preenchimento desses valores com estimativas apropriadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf128e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Selecionar colunas num√©ricas automaticamente\n",
    "numeric_cols = housing.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# 2. Configura√ß√£o do gr√°fico\n",
    "n_cols = 3  # N√∫mero de colunas no grid\n",
    "n_rows = len(numeric_cols) // n_cols  # Calcula linhas necess√°rias\n",
    "\n",
    "plt.figure(figsize=(14, 4*n_rows))  # Ajuste autom√°tico de altura\n",
    "\n",
    "# 3. Criar histogramas para cada vari√°vel\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    # Histograma com KDE\n",
    "    sns.histplot(housing[col], \n",
    "                 bins=30, \n",
    "                 kde=True, \n",
    "                 color='#1f77b4',  # Azul matplotlib\n",
    "                 edgecolor='white',\n",
    "                 linewidth=0.5)\n",
    "    \n",
    "    # Customiza√ß√£o\n",
    "    plt.title(f'Distribui√ß√£o de {col}', fontsize=12, pad=10)\n",
    "    plt.xlabel('Valor', fontsize=10)\n",
    "    plt.ylabel('Frequ√™ncia', fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar linhas de refer√™ncia\n",
    "    plt.axvline(housing[col].mean(), color='red', linestyle='--', linewidth=1, label='M√©dia')\n",
    "    plt.axvline(housing[col].median(), color='green', linestyle='-', linewidth=1, label='Mediana')\n",
    "    \n",
    "    if i == 1:  # Legenda apenas no primeiro gr√°fico\n",
    "        plt.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Espa√ßamento entre subplots\n",
    "plt.suptitle('Distribui√ß√£o das Vari√°veis Num√©ricas', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f1449",
   "metadata": {},
   "source": [
    "Podemos observar distribui√ß√µes assim√©tricas em todos os gr√°ficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e77597",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5*n_rows))\n",
    "plt.suptitle('An√°lise de Outliers - Box Plots', y=1.02, fontsize=14)\n",
    "\n",
    "# 3. Criar box plots para cada vari√°vel\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    # Box plot customizado\n",
    "    sns.boxplot(y=housing[col], \n",
    "                color='#4C72B0', \n",
    "                width=0.5,\n",
    "                flierprops=dict(marker='o', \n",
    "                               markersize=5,\n",
    "                               markerfacecolor='#DD8452',\n",
    "                               markeredgecolor='none',\n",
    "                               alpha=0.7))\n",
    "    \n",
    "    # Adicionar linha da mediana\n",
    "    median = housing[col].median()\n",
    "    plt.axhline(median, color='#55A868', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Customiza√ß√£o\n",
    "    plt.title(col, fontsize=12, pad=10)\n",
    "    plt.ylabel('Valores', fontsize=9)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Anotar estat√≠sticas\n",
    "    stats = housing[col].describe()\n",
    "    textstr = f\"Mediana: {median:.2f}\\nQ1: {stats['25%']:.2f}\\nQ3: {stats['75%']:.2f}\"\n",
    "    plt.text(0.05, 0.95, textstr, \n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize=8,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98b901",
   "metadata": {},
   "source": [
    "## Limpeza e Tratamento dos Dados\n",
    "\n",
    "Pacotes usados:\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# üì¶ Importando a biblioteca necess√°ria\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import necess√°rio\n",
    "from sklearn.model_selection import train_test_split\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea025b",
   "metadata": {},
   "source": [
    "### **Data Imputation**\n",
    "\n",
    "Uma decis√£o crucial que voc√™ deve tomar √© o que fazer com os dados ausentes. Voc√™ pode optar por remover as linhas ou colunas que cont√™m valores ausentes, ou pode preencher esses valores com a m√©dia, mediana ou outro valor apropriado. A escolha depende do contexto dos dados e do impacto que os valores ausentes podem ter na an√°lise. Para esta oficina, vamos optar por preencher os valores ausentes (NAs) com algum estimador, por√©m vamos mostrar como remover as linhas ou colunas com NAs tamb√©m.\n",
    "\n",
    "Antes disso, temos que entender o que √© imputa√ß√£o de dados. A imputa√ß√£o de dados √© o processo de substituir valores ausentes em um conjunto de dados por valores estimados. Isso √© importante porque muitos algoritmos de aprendizado de m√°quina n√£o podem lidar com dados ausentes e, portanto, a imputa√ß√£o √© uma etapa crucial no pr√©-processamento dos dados.\n",
    "A imputa√ß√£o pode ser feita de v√°rias maneiras, incluindo:\n",
    "- **M√©dia/Mediana/Moda:** Substituir valores ausentes pela m√©dia, mediana ou moda da coluna.\n",
    "- **KNNImputer:** Usar o algoritmo K-Nearest Neighbors para prever valores ausentes com base em valores de inst√¢ncias semelhantes.\n",
    "- **Regress√£o:** Usar um modelo de regress√£o para prever valores ausentes com base em outras vari√°veis.\n",
    "\n",
    "#### üéØ Design do Scikit-Learn\n",
    "\n",
    "A API do Scikit-Learn √© notavelmente bem projetada. Estes s√£o os principais princ√≠pios de design:\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Consist√™ncia\n",
    "\n",
    "Todos os objetos compartilham uma interface consistente e simples.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Estimadores**\n",
    "\n",
    "Qualquer objeto que possa estimar alguns par√¢metros com base em um conjunto de dados √© chamado de **estimador** (por exemplo, um `SimpleImputer` √© um estimador).  \n",
    "\n",
    "- A estimativa √© realizada pelo m√©todo `fit()`, que recebe um conjunto de dados como par√¢metro.  \n",
    "- Para algoritmos de aprendizado supervisionado, o `fit()` recebe dois conjuntos de dados: um com as amostras e outro com os r√≥tulos (*labels*).  \n",
    "- Qualquer outro par√¢metro necess√°rio para orientar o processo de estimativa √© chamado de **hiperpar√¢metro** (por exemplo, a estrat√©gia do `SimpleImputer`), e deve ser definido como uma vari√°vel de inst√¢ncia (geralmente por meio de um par√¢metro do construtor).\n",
    "\n",
    "---\n",
    "\n",
    "##### **Transformadores**\n",
    "\n",
    "Alguns estimadores (como o `SimpleImputer`) tamb√©m podem **transformar** um conjunto de dados; estes s√£o chamados de **transformadores**.\n",
    "\n",
    "- A transforma√ß√£o √© feita pelo m√©todo `transform()`, que recebe o conjunto de dados a ser transformado e retorna o conjunto transformado.  \n",
    "- Essa transforma√ß√£o geralmente depende dos par√¢metros aprendidos durante o `fit()`, como acontece com o `SimpleImputer`.  \n",
    "- Todos os transformadores possuem tamb√©m o m√©todo `fit_transform()`, que √© equivalente a chamar `fit()` seguido de `transform()`.  \n",
    "  - **Obs:** em alguns casos, `fit_transform()` √© otimizado e executa muito mais r√°pido do que chamar os dois m√©todos separadamente.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Preditores**\n",
    "\n",
    "Alguns estimadores s√£o capazes de fazer **previs√µes**; estes s√£o chamados de **preditores**.\n",
    "\n",
    "- Por exemplo, o modelo `LinearRegression` √© um preditor: dado o PIB per capita de um pa√≠s, ele prev√™ o n√≠vel de satisfa√ß√£o com a vida.  \n",
    "- Um preditor possui o m√©todo `predict()`, que recebe um conjunto de novas inst√¢ncias e retorna as previs√µes correspondentes.  \n",
    "- Ele tamb√©m possui o m√©todo `score()`, que mede a qualidade das previs√µes, dado um conjunto de teste (e os r√≥tulos correspondentes, no caso de aprendizado supervisionado).\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Inspe√ß√£o\n",
    "\n",
    "- Todos os **hiperpar√¢metros** de um estimador s√£o acess√≠veis diretamente via vari√°veis p√∫blicas de inst√¢ncia (por exemplo, `imputer.strategy`).  \n",
    "- Todos os **par√¢metros aprendidos** s√£o acess√≠veis via vari√°veis p√∫blicas de inst√¢ncia com um **sufixo de sublinhado** (por exemplo, `imputer.statistics_`).\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ N√£o prolifera√ß√£o de classes\n",
    "\n",
    "- Conjuntos de dados s√£o representados como arrays do **NumPy** ou matrizes esparsas do **SciPy**, em vez de classes personalizadas.  \n",
    "- Hiperpar√¢metros s√£o apenas strings ou n√∫meros comuns do Python.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Composi√ß√£o\n",
    "\n",
    "- Blocos de constru√ß√£o existentes s√£o reutilizados sempre que poss√≠vel.  \n",
    "- Por exemplo, √© f√°cil criar um **Pipeline** (fluxo de processamento) a partir de uma sequ√™ncia arbitr√°ria de transformadores seguida por um estimador final.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Valores padr√£o sensatos\n",
    "\n",
    "- O Scikit-Learn fornece valores padr√£o razo√°veis para a maioria dos par√¢metros.  \n",
    "- Isso facilita a cria√ß√£o r√°pida de um sistema funcional b√°sico (*baseline*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc15504",
   "metadata": {},
   "source": [
    "#### 1¬∞ Op√ß√£o: Remover inst√¢ncias com NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um novo DataFrame chamado 'housing_not_na' onde as linhas que possuem\n",
    "# valores nulos (NaN) na coluna 'total_bedrooms' s√£o removidas.\n",
    "\n",
    "housing_not_na = housing.dropna(\n",
    "    subset=['total_bedrooms'],  # Define que a verifica√ß√£o de nulos ser√° feita apenas na coluna 'total_bedrooms'.\n",
    "    inplace=False                # inplace=False garante que o DataFrame original (housing) n√£o ser√° modificado,\n",
    "                                 # e sim retornar√° um novo DataFrame com as linhas sem nulos nessa coluna.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13146aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing_not_na).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9be4b",
   "metadata": {},
   "source": [
    "#### 2¬∞ Op√ß√£o: Remover colunas com NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um novo DataFrame chamado 'housing_not_na' removendo a coluna 'total_bedrooms' do DataFrame original.\n",
    "\n",
    "housing_not_na = housing.drop(\n",
    "    'total_bedrooms',  # Especifica qual coluna ser√° removida. Neste caso, 'total_bedrooms'.\n",
    "    axis=1,             # axis=1 indica que estamos removendo uma COLUNA (se fosse axis=0, seria uma LINHA).\n",
    "    inplace=False       # inplace=False garante que a opera√ß√£o n√£o altera o DataFrame original (housing),\n",
    "                        # mas retorna um novo DataFrame com a coluna removida.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99707e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalyzer(housing_not_na).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79757f",
   "metadata": {},
   "source": [
    "#### 3¬∞ Op√ß√£o: Preencher NAs com a alguma medida de tend√™ncia central da coluna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45464627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Calculando a m√©dia da coluna 'total_bedrooms'\n",
    "# A fun√ß√£o .mean() calcula a m√©dia aritm√©tica dos valores num√©ricos, ignorando automaticamente os valores NaN\n",
    "mean = housing['total_bedrooms'].mean()\n",
    "\n",
    "# ‚úÖ Fazendo uma c√≥pia do dataframe original para n√£o alterar os dados originais\n",
    "housing_mean = housing.copy()\n",
    "\n",
    "# ‚úÖ Substituindo os valores ausentes (NaN) da coluna 'total_bedrooms' pela m√©dia calculada\n",
    "# A fun√ß√£o .fillna(mean) preenche todas as c√©lulas que est√£o com NaN com o valor da m√©dia\n",
    "housing_mean['total_bedrooms'] = housing_mean['total_bedrooms'].fillna(mean)\n",
    "\n",
    "# ‚úÖ Verificando quantos valores ausentes ainda existem na coluna 'total_bedrooms' ap√≥s a imputa√ß√£o\n",
    "# A fun√ß√£o .isnull().sum() retorna a quantidade de valores que ainda s√£o NaN (se tudo deu certo, deve ser zero)\n",
    "na_mean = housing_mean['total_bedrooms'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51484351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a mediana da coluna 'total_bedrooms' do DataFrame 'housing'\n",
    "# A mediana √© uma medida de tend√™ncia central que √© menos sens√≠vel a valores extremos do que a m√©dia.\n",
    "median = housing['total_bedrooms'].median()\n",
    "\n",
    "# Cria uma c√≥pia do DataFrame original 'housing' para n√£o modificar os dados originais\n",
    "housing_median = housing.copy()\n",
    "\n",
    "# Substitui os valores ausentes (NaN) da coluna 'total_bedrooms' pela mediana calculada\n",
    "# O m√©todo fillna() preenche valores faltantes, garantindo que n√£o haja dados ausentes nessa coluna.\n",
    "housing_median['total_bedrooms'] = housing_median['total_bedrooms'].fillna(median)\n",
    "\n",
    "# Verifica quantos valores ausentes (NaN) ainda existem na coluna 'total_bedrooms' ap√≥s a substitui√ß√£o\n",
    "# O m√©todo isnull() identifica valores nulos e sum() faz a contagem total deles.\n",
    "na_median = housing_median['total_bedrooms'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sa√≠da em Markdown\n",
    "display(Markdown(f\"\"\"\n",
    "## üîß Resultado da Imputa√ß√£o de Dados Nulos\n",
    "\n",
    "- üß† Ap√≥s preencher com **M√©dia**, restam **{na_mean}** valores nulos na coluna `total_bedrooms`.\n",
    "- üß† Ap√≥s preencher com **Mediana**, restam **{na_median}** valores nulos na coluna `total_bedrooms`.\n",
    "\n",
    "‚úÖ Ambos os m√©todos resolveram os dados faltantes, caso o n√∫mero seja zero.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b22900",
   "metadata": {},
   "source": [
    "Esse processo pode ser automatizado com o uso de bibliotecas como o **Scikit-learn**, que fornece uma classe √∫til que se encarrega de valores ausentes: a ``SimpleImputer``. Essa classe pode ser usada para preencher valores ausentes com a m√©dia, mediana ou moda de uma coluna. Vejamos como us√°-la: primeiro, voc√™ precisa criar uma inst√¢ncia da ``SimpleImputer``, especificando que deseja substituir os valores ausentes de cada atributo pela m√©dia/mediana desse atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71118258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696bfd5",
   "metadata": {},
   "source": [
    "A ``imputer`` simplesmente calcula a m√©dia de cada coluna e armazena esses valores em sua vari√°vel ``statistics_``. Somente o atributo `total_bedrooms` tem valores ausentes, ent√£o o imputer calcula a m√©dia apenas desse atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.mean().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2641c08",
   "metadata": {},
   "source": [
    "Agora, voc√™ pode usar o m√©todo ``transform()`` para preencher os valores ausentes com a m√©dia de cada coluna. O m√©todo ``transform()`` retorna um novo array NumPy, que cont√©m os dados preenchidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c31528",
   "metadata": {},
   "source": [
    "Voc√™ pode usar o m√©todo ``fit_transform()`` para fazer isso em uma √∫nica etapa tamb√©m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = imputer.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0759c7",
   "metadata": {},
   "source": [
    "#### 4¬∞ Op√ß√£o: Preencher NAs com a estima√ß√£o de um modelo\n",
    "\n",
    "para essa caso usaremos o KNNImputer do Scikit-learn. O KNNImputer √© uma t√©cnica de imputa√ß√£o que utiliza o algoritmo K-Nearest Neighbors (KNN) para preencher valores ausentes em um conjunto de dados. Ele funciona identificando os vizinhos mais pr√≥ximos de uma inst√¢ncia com dados ausentes e usando os valores desses vizinhos para estimar o valor ausente. Essa abordagem √© √∫til quando os dados t√™m uma estrutura espacial ou temporal, pois considera a similaridade entre as inst√¢ncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b28ed",
   "metadata": {},
   "source": [
    "#### üî¢ Demonstra√ß√£o Matem√°tica do KNN e do KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510f572",
   "metadata": {},
   "source": [
    "##### üìö **Teoria do KNN (K-Nearest Neighbors)**\n",
    "\n",
    "O algoritmo KNN √© um m√©todo baseado em inst√¢ncias usado tanto para **classifica√ß√£o quanto regress√£o**, onde uma amostra desconhecida √© classificada ou recebe um valor estimado com base nos seus **K vizinhos mais pr√≥ximos** no espa√ßo de caracter√≠sticas.\n",
    "\n",
    "A base matem√°tica do KNN se fundamenta na ideia de **dist√¢ncias** no espa√ßo vetorial, mais frequentemente utilizando a **Dist√¢ncia Euclidiana**, embora outras m√©tricas tamb√©m possam ser aplicadas (Manhattan, Minkowski, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "##### üîó **Equa√ß√£o da Dist√¢ncia Euclidiana**\n",
    "\n",
    "Para dois pontos $X = (x_1, x_2, ..., x_n)$ e $Y = (y_1, y_2, ..., y_n)$ em um espa√ßo $n$-dimensional, a dist√¢ncia euclidiana √© calculada como:\n",
    "\n",
    "$\n",
    "d(X, Y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$\n",
    "\n",
    "Essa √© a m√©trica padr√£o no KNN, pois mede a \"reta\" que liga dois pontos no espa√ßo.\n",
    "\n",
    "---\n",
    "\n",
    "##### üö© **Processo Matem√°tico do KNN**\n",
    "\n",
    "1. Calcular a dist√¢ncia entre o ponto com valor desconhecido e todos os outros pontos do conjunto de dados.\n",
    "   \n",
    "2. Ordenar os dados com base na menor dist√¢ncia.\n",
    "\n",
    "3. Selecionar os **K vizinhos mais pr√≥ximos**.\n",
    "\n",
    "4. - **Regress√£o:** Calcular a **m√©dia** dos valores da vari√°vel alvo dos K vizinhos:\n",
    "\n",
    "   $\n",
    "   \\hat{y} = \\dfrac{\\sum_{i=1}^{K} y_i}{K}\n",
    "   $\n",
    "\n",
    "   - **Classifica√ß√£o:** Selecionar a **classe mais frequente** entre os vizinhos.\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß **Teoria do KNNImputer para Dados Faltantes**\n",
    "\n",
    "O **KNNImputer** aplica exatamente o mesmo conceito do KNN, mas ao inv√©s de prever uma vari√°vel alvo externa, ele preenche os valores **faltantes nas pr√≥prias colunas do dataset**.\n",
    "\n",
    "- Para cada c√©lula com valor ausente:\n",
    "  1. Localizam-se os **K registros mais pr√≥ximos** (com base nas outras colunas que t√™m valores presentes).\n",
    "  2. Calcula-se a **m√©dia dos valores** dos vizinhos na coluna com dados ausentes.\n",
    "  3. Substitui-se o valor nulo pela m√©dia calculada.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚öôÔ∏è **F√≥rmula da Imputa√ß√£o**\n",
    "\n",
    "Dado um valor ausente na coluna $j$ do ponto $x$, o valor imputado √©:\n",
    "\n",
    "$\n",
    "\\hat{x}_j = \\frac{\\sum_{i=1}^{K} x_{ij}}{K}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $x_{ij}$ = valor da coluna $j$ no vizinho $i$.\n",
    "- $K$ = n√∫mero de vizinhos considerados.\n",
    "\n",
    "O processo √© repetido para cada valor ausente, considerando as dist√¢ncias calculadas **apenas nas colunas que n√£o possuem valores ausentes simultaneamente**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f84064",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è **Exemplo Manual (Feito na m√£o)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d182772",
   "metadata": {},
   "source": [
    "Imagine um dataset simplificado com 3 registros e 3 vari√°veis ($A$, $B$ e $C$):\n",
    "\n",
    "| Registro | A   | B   | C   |\n",
    "|----------|-----|-----|-----|\n",
    "| 1        | 1.0 | 2.0 | 5.0 |\n",
    "| 2        | 2.0 | NaN | 7.0 |\n",
    "| 3        | 3.0 | 6.0 | 9.0 |\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úîÔ∏è **Passo 1:** Queremos imputar o valor faltante na linha 2, coluna **B**.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úîÔ∏è **Passo 2:** Calculamos a dist√¢ncia da linha 2 para as outras linhas utilizando as colunas **A** e **C**, que est√£o completas.\n",
    "\n",
    "- **Dist√¢ncia para linha 1:**\n",
    "\n",
    "$\n",
    "d = \\sqrt{(2 - 1)^2 + (7 - 5)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.24\n",
    "$\n",
    "\n",
    "- **Dist√¢ncia para linha 3:**\n",
    "\n",
    "$\n",
    "d = \\sqrt{(2 - 3)^2 + (7 - 9)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.24\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úîÔ∏è **Passo 3:** Selecionamos os $K = 2$ vizinhos mais pr√≥ximos (linha 1 e linha 3).\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úîÔ∏è **Passo 4:** Pegamos os valores da coluna **B** dos vizinhos:\n",
    "\n",
    "- Linha 1 ‚Üí **B = 2.0**\n",
    "- Linha 3 ‚Üí **B = 6.0**\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úîÔ∏è **Passo 5:** Calculamos a m√©dia dos vizinhos:\n",
    "\n",
    "$\n",
    "\\hat{B} = \\frac{2.0 + 6.0}{2} = 4.0\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úîÔ∏è **Resultado:** O valor **NaN** na linha 2, coluna **B**, ser√° preenchido com **4.0**.\n",
    "\n",
    "---\n",
    "\n",
    "##### üî• **Vantagens do KNNImputer**\n",
    "\n",
    "- ‚úÖ Leva em considera√ß√£o a estrutura dos dados.\n",
    "- ‚úÖ Mais robusto do que imputa√ß√£o por m√©dia ou mediana simples.\n",
    "- ‚úÖ Considera rela√ß√µes n√£o lineares entre as vari√°veis.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚ö†Ô∏è **Desvantagens**\n",
    "\n",
    "- üö´ Alto custo computacional em datasets grandes.\n",
    "- üö´ Sens√≠vel √† escolha de $K$ (um $K$ muito pequeno ou muito grande pode distorcer os resultados).\n",
    "- üö´ Dependente da escala das vari√°veis (precisa de **normaliza√ß√£o ou padroniza√ß√£o** antes de aplicar, devido √† dist√¢ncia Euclidiana ser sens√≠vel √† escala).\n",
    "\n",
    "---\n",
    "\n",
    "##### üí° **Observa√ß√£o Importante**\n",
    "\n",
    "‚úîÔ∏è Antes de aplicar o **KNNImputer**, √© **fundamental normalizar ou padronizar os dados**, pois as dist√¢ncias podem ser distorcidas se as vari√°veis estiverem em escalas muito diferentes.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìñ **Conclus√£o**\n",
    "\n",
    "O KNNImputer √© uma t√©cnica poderosa e intuitiva de imputa√ß√£o que funciona bem quando h√° rela√ß√µes estruturais nos dados. Entretanto, precisa ser usada com cuidado em rela√ß√£o √† escolha de $K$ e √† normaliza√ß√£o dos dados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403245a5",
   "metadata": {},
   "source": [
    "#### Continua√ß√£o da 4¬∞ Op√ß√£o\n",
    "\n",
    "Agora vamos **aplicar o `KNNImputer`** ao nosso conjunto de dados.  \n",
    "\n",
    "O `KNNImputer` √© uma t√©cnica de imputa√ß√£o que preenche valores ausentes com base na m√©dia (ou outro crit√©rio) dos **valores mais pr√≥ximos** ‚Äî ou seja, ele busca os **k vizinhos mais pr√≥ximos** de cada amostra incompleta e utiliza esses vizinhos para estimar o valor faltante.  \n",
    "\n",
    "Dessa forma, a imputa√ß√£o leva em conta a **similaridade entre as amostras**, resultando em uma abordagem mais robusta do que simplesmente substituir por m√©dias ou medianas globais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83593cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Selecionando apenas vari√°veis num√©ricas do dataframe\n",
    "# Isso √© necess√°rio porque o KNNImputer trabalha apenas com vari√°veis num√©ricas\n",
    "data = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "# ‚úÖ Separando os dados em dois conjuntos:\n",
    "# 1. Dados COM valores na coluna 'total_bedrooms' (CNA = Complete No NA)\n",
    "#    -> Usado para treino e valida√ß√£o do modelo de imputa√ß√£o\n",
    "data_cna = data.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# 2. Dados COM valores ausentes na coluna 'total_bedrooms'\n",
    "#    -> Este ser√° o conjunto onde aplicaremos o modelo treinado para imputar os NAs reais\n",
    "data_na = data[data['total_bedrooms'].isna()]\n",
    "\n",
    "# ‚úÖ Verificando o tamanho dos dois conjuntos\n",
    "print(f\"Shape dos dados completos (CNA): {data_cna.shape}\")\n",
    "print(f\"Shape dos dados com NA em total_bedrooms: {data_na.shape}\")\n",
    "\n",
    "# ‚úÖ Reduzindo o dataset completo (CNA) para 8.000 observa√ß√µes de forma aleat√≥ria\n",
    "# Isso √© √∫til para acelerar o processamento e testes\n",
    "# np.random.seed(42) define uma semente aleat√≥ria para garantir que os resultados sejam reproduz√≠veis\n",
    "np.random.seed(42)\n",
    "data_cna_reduzido = data_cna.sample(n=8000)\n",
    "\n",
    "# ‚úÖ Confirmando o tamanho do dataset reduzido\n",
    "print(f\"Shape dos dados CNA reduzidos: {data_cna_reduzido.shape}\")\n",
    "\n",
    "# ‚úÖ Definindo as vari√°veis para a modelagem:\n",
    "# X -> todas as vari√°veis independentes (exceto 'total_bedrooms')\n",
    "# y -> vari√°vel dependente, que ser√° imputada (total_bedrooms)\n",
    "X = data_cna_reduzido.drop(columns=['total_bedrooms'])\n",
    "y = data_cna_reduzido['total_bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Elbow Method com Valida√ß√£o Cruzada (4 Folds) para KNN Imputer\n",
    "\n",
    "# Lista para armazenar os RMSE m√©dios de cada valor de K testado\n",
    "rmse_values = []\n",
    "\n",
    "# Definindo os valores de K que ser√£o testados (de 1 a 10)\n",
    "k_values = range(1, 11)\n",
    "\n",
    "# üîÑ Definindo o m√©todo de Valida√ß√£o Cruzada:\n",
    "# - n_splits=4 ‚Üí divide o dataset em 4 partes\n",
    "# - shuffle=True ‚Üí embaralha os dados antes de dividir (garante aleatoriedade)\n",
    "# - random_state=42 ‚Üí fixa a semente para garantir resultados reproduz√≠veis\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop externo: testa cada valor de K (n√∫mero de vizinhos)\n",
    "for k in k_values:\n",
    "    fold_rmse = []  # Lista para armazenar os RMSE de cada fold (valida√ß√£o)\n",
    "\n",
    "    # Loop interno: executa a valida√ß√£o cruzada (4 folds)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # üîß Separando os dados de treino e teste com base nos √≠ndices dos folds\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # üõ†Ô∏è Criando os dataframes de treino e teste com a coluna alvo 'total_bedrooms'\n",
    "        # Dados de treino possuem 'total_bedrooms' conhecido\n",
    "        train_data = X_train.copy()\n",
    "        train_data['total_bedrooms'] = y_train\n",
    "\n",
    "        # Dados de teste simulam NA na coluna alvo (como se precis√°ssemos imputar)\n",
    "        test_data = X_test.copy()\n",
    "        test_data['total_bedrooms'] = np.nan\n",
    "\n",
    "        # üîó Concatenando treino + teste para que o KNN Imputer busque os vizinhos no conjunto inteiro\n",
    "        combined = pd.concat([train_data, test_data])\n",
    "\n",
    "        # üî• Escalonamento:\n",
    "        # - O KNN √© sens√≠vel √†s escalas das vari√°veis.\n",
    "        # - StandardScaler padroniza os dados para m√©dia 0 e desvio padr√£o 1.\n",
    "        scaler = StandardScaler()\n",
    "        combined_scaled = scaler.fit_transform(combined)\n",
    "\n",
    "        # üöÄ Aplicando KNN Imputer:\n",
    "        # - n_neighbors=k ‚Üí n√∫mero de vizinhos considerados para imputa√ß√£o\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputed = imputer.fit_transform(combined_scaled)\n",
    "\n",
    "        # üîç Separando os dados de teste imputados:\n",
    "        # - imputed[-len(X_test):, -1]:\n",
    "        #   -> Seleciona as √∫ltimas 'len(X_test)' linhas do array 'imputed'.\n",
    "        #   -> O √≠ndice negativo -len(X_test) significa: \"comece a selecionar a partir desta posi√ß√£o contando de tr√°s pra frente\".\n",
    "        #   -> Isso garante que estamos pegando exatamente as linhas do conjunto de teste, assumindo que ele foi concatenado no final.\n",
    "        #   -> O -1 seleciona a √∫ltima coluna ('total_bedrooms').\n",
    "        imputed_test = imputed[-len(X_test):, -1]\n",
    "\n",
    "        # ‚úÖ Calculando o RMSE (Root Mean Squared Error - Erro Quadr√°tico M√©dio):\n",
    "        # - Antes de comparar as previs√µes (imputed_test) com os valores reais (y_test),\n",
    "        #   √© necess√°rio DESFAZER o escalonamento que foi aplicado anteriormente com o StandardScaler.\n",
    "        # \n",
    "        # - F√≥rmula para reverter o StandardScaler:\n",
    "        #     valor_original = valor_escalado * desvio_padrao + media\n",
    "        #\n",
    "        # - Aqui usamos:\n",
    "        #   scaler.scale_[-1]: seleciona o desvio padr√£o da √∫ltima coluna ('total_bedrooms').   \n",
    "        #   scaler.mean_[-1]: seleciona a m√©dia da √∫ltima coluna ('total_bedrooms').\n",
    "        #\n",
    "        # ‚úÖ Por que usamos o √≠ndice -1?\n",
    "        #   -> O √≠ndice -1 sempre aponta para o √∫ltimo elemento de uma lista ou array.\n",
    "        #   -> Como 'total_bedrooms' √© a √∫ltima coluna do dataset transformado, pegamos os par√¢metros de escalonamento\n",
    "        #      (desvio padr√£o e m√©dia) correspondentes a essa coluna.\n",
    "        #\n",
    "        # ‚ùó Se us√°ssemos √≠ndices positivos, ter√≠amos que contar a posi√ß√£o exata, o que √© mais propenso a erro\n",
    "        #     se o n√∫mero ou a ordem das colunas mudar.\n",
    "        #\n",
    "        # - Ap√≥s desfazer o escalonamento, calculamos o RMSE para avaliar o erro da imputa√ß√£o.\n",
    "        rmse = np.sqrt(mean_squared_error(\n",
    "            y_test, \n",
    "            imputed_test * scaler.scale_[-1] + scaler.mean_[-1]  # Desfazendo o escalonamento da √∫ltima coluna\n",
    "        ))\n",
    "\n",
    "        # Armazena o RMSE desse fold\n",
    "        fold_rmse.append(rmse)\n",
    "\n",
    "    # üìä Ap√≥s os 4 folds, calcula o RMSE m√©dio para o valor atual de K\n",
    "    avg_rmse = np.mean(fold_rmse)\n",
    "\n",
    "    # Salva o RMSE m√©dio na lista geral\n",
    "    rmse_values.append(avg_rmse)\n",
    "\n",
    "    # Exibe o resultado na tela\n",
    "    print(f\"K={k}: RMSE m√©dio={avg_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Cria√ß√£o do gr√°fico Elbow Method para visualizar o RMSE em fun√ß√£o do n√∫mero de vizinhos (K)\n",
    "\n",
    "# üîß Define o tamanho da figura do gr√°fico\n",
    "plt.figure(figsize=(10, 6))  # Largura=10, Altura=6\n",
    "\n",
    "# ü™¢ Plota os valores de RMSE m√©dio para cada K:\n",
    "# - k_values ‚Üí eixo X (n√∫mero de vizinhos)\n",
    "# - rmse_values ‚Üí eixo Y (erro m√©dio quadr√°tico para cada K)\n",
    "# - marker='o' ‚Üí adiciona bolinhas nos pontos para destacar\n",
    "plt.plot(k_values, rmse_values, marker='o')\n",
    "\n",
    "# üé® T√≠tulo do gr√°fico\n",
    "plt.title('Elbow Method para KNNImputer (com Escalonamento)')\n",
    "\n",
    "# üè∑Ô∏è Nome dos eixos\n",
    "plt.xlabel('N√∫mero de Vizinhos (K)')\n",
    "plt.ylabel('RMSE M√©dio')\n",
    "\n",
    "# üó∫Ô∏è Adiciona uma grade (linhas de refer√™ncia no fundo do gr√°fico)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d81084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîé Encontrando o melhor valor de K baseado no menor RMSE\n",
    "\n",
    "# np.argmin(rmse_values) retorna o √≠ndice do menor valor dentro da lista rmse_values\n",
    "# k_values √© uma sequ√™ncia (range) com os valores testados de K\n",
    "# Ent√£o, usamos esse √≠ndice para acessar o k correspondente no k_values\n",
    "\n",
    "best_k = k_values[np.argmin(rmse_values)]  # Seleciona o K que teve o menor RMSE\n",
    "\n",
    "# Exibe o resultado para o usu√°rio\n",
    "print(f\"Melhor valor de K: {best_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Separando as vari√°veis num√©ricas e categ√≥ricas\n",
    "# Seleciona apenas as colunas num√©ricas do dataframe\n",
    "numericas = housing.select_dtypes(include=[np.number])\n",
    "\n",
    "# Seleciona as colunas categ√≥ricas que voc√™ quer manter para depois juntar (ex: 'ocean_proximity')\n",
    "categoricas = housing[['ocean_proximity']]  # Se houver mais vari√°veis categ√≥ricas, adiciona na lista\n",
    "\n",
    "# üîß Separando os dados num√©ricos:\n",
    "# 'numericas_cna' cont√©m as linhas onde 'total_bedrooms' N√ÉO tem valores faltantes (completo)\n",
    "numericas_cna = numericas.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# 'numericas_na' cont√©m as linhas onde 'total_bedrooms' est√° faltando (NaN)\n",
    "numericas_na = numericas[numericas['total_bedrooms'].isna()]\n",
    "\n",
    "# üîó Agora concatenamos as duas partes para montar o dataset completo de vari√°veis num√©ricas,\n",
    "# onde as linhas com 'total_bedrooms' faltando v√£o estar no final\n",
    "full_numericas = pd.concat([numericas_cna, numericas_na])\n",
    "\n",
    "# üî• Escalonando as vari√°veis num√©ricas para que todas fiquem na mesma escala\n",
    "# Isso evita que vari√°veis com valores muito maiores dominem o c√°lculo da dist√¢ncia no KNN\n",
    "scaler = StandardScaler()\n",
    "full_scaled = scaler.fit_transform(full_numericas)\n",
    "\n",
    "# üöÄ Aplicando o KNNImputer com o n√∫mero √≥timo de vizinhos (K=4 no exemplo)\n",
    "# O imputador vai preencher os valores faltantes baseando-se nos 4 vizinhos mais pr√≥ximos\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "imputed_data = imputer.fit_transform(full_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a240e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîô Desfazendo o escalonamento\n",
    "# imputed_data √© um array numpy com os dados ap√≥s imputa√ß√£o, mas ainda padronizados (z-score)\n",
    "# scaler.scale_ √© um array com o desvio padr√£o de cada coluna calculado no fit do StandardScaler\n",
    "# scaler.mean_ √© um array com a m√©dia de cada coluna calculada no fit do StandardScaler\n",
    "# Para voltar aos valores originais (antes do escalonamento), aplicamos a f√≥rmula inversa do z-score:\n",
    "# valor_original = valor_padronizado * desvio_padr√£o + m√©dia\n",
    "imputed_data = imputed_data * scaler.scale_ + scaler.mean_\n",
    "\n",
    "\n",
    "# üîß Convertendo o resultado imputado (array NumPy) de volta para DataFrame pandas\n",
    "# columns=full_numericas.columns -> mantemos os nomes originais das colunas\n",
    "# index=full_numericas.index -> mantemos os √≠ndices originais (linhas)\n",
    "# Isso √© importante para manter o alinhamento e facilitar manipula√ß√µes futuras\n",
    "imputed_numericas = pd.DataFrame(imputed_data, columns=full_numericas.columns, index=full_numericas.index)\n",
    "\n",
    "\n",
    "# ‚úÖ Juntando as vari√°veis categ√≥ricas (n√£o num√©ricas) que foram separadas antes\n",
    "# pd.concat() concatena DataFrames pelo eixo das colunas (axis=1)\n",
    "# Isso garante que todas as vari√°veis (num√©ricas + categ√≥ricas) estejam no mesmo DataFrame final\n",
    "imputed_final = pd.concat([imputed_numericas, categoricas], axis=1)\n",
    "\n",
    "\n",
    "# Executa uma an√°lise para verificar se existem valores ausentes (NaN) no DataFrame final\n",
    "# DataAnalyzer √© uma classe customizada que tem um m√©todo analise_dados_nulos()\n",
    "DataAnalyzer(imputed_final).analise_dados_nulos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÅ Dados finais prontos\n",
    "housing_imputed = imputed_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43cd3a",
   "metadata": {},
   "source": [
    "#### Comparar o desempenho do KNN-Imputer com os datasets NA, m√©dia e mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_idx = housing['total_bedrooms'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d89506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √çndices onde total_bedrooms era NA no dataset original\n",
    "na_idx = housing['total_bedrooms'].isna()\n",
    "\n",
    "# Datasets para plotar (com total_bedrooms e median_house_value)\n",
    "datasets = {\n",
    "    'Original (com NA)': housing[['total_bedrooms', 'median_house_value']],\n",
    "    'Imputa√ß√£o pela m√©dia': housing_mean[['total_bedrooms', 'median_house_value']],\n",
    "    'Imputa√ß√£o pela mediana': housing_median[['total_bedrooms', 'median_house_value']],\n",
    "    'Imputa√ß√£o KNN': imputed_final[['total_bedrooms', 'median_house_value']],\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i, (name, df) in enumerate(datasets.items(), start=1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    \n",
    "    # Plotar todos os pontos em azul claro\n",
    "    plt.scatter(df['total_bedrooms'], df['median_house_value'], s=10, alpha=0.4, label='Dados Originais')\n",
    "    \n",
    "    # Se for um m√©todo que imputou, destacar os valores imputados\n",
    "    if name in ['Imputa√ß√£o pela m√©dia', 'Imputa√ß√£o pela mediana', 'Imputa√ß√£o KNN']:\n",
    "        # Pega os dados imputados (nas posi√ß√µes de NA no original)\n",
    "        imputados_x = df.loc[na_idx, 'total_bedrooms']\n",
    "        imputados_y = df.loc[na_idx, 'median_house_value']\n",
    "        \n",
    "        # Plota os pontos imputados em vermelho, maior e com transpar√™ncia menor para destacar\n",
    "        plt.scatter(imputados_x, imputados_y, color='red', s=30, alpha=0.7, label='Valores Imputados')\n",
    "    \n",
    "    plt.title(name)\n",
    "    plt.xlabel('Total Bedrooms')\n",
    "    plt.ylabel('Median House Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a57e37",
   "metadata": {},
   "source": [
    "### **Transformadores de Dados**\n",
    "\n",
    "Embora o Sklearn tenha muitos transformadores, n√≥s precisamos escrever nossos pr√≥rpios transformadores para tarefas como opera√ß√µes de pr√©-processamento, como normaliza√ß√£o, padroniza√ß√£o, transforma√ß√£o de vari√°veis categ√≥ricas em vari√°veis num√©ricas, etc. Queremos que nosso transformador funcione perfeitamente com as funcionalidades do Sklearn, como Pipelines e GridSearchCV. Para isso, precisamos s√≥ precisamos criar uma classe e implementar os m√©todos ``fit()``, ``transform()`` e ``fit_transform()``. Para obter o √∫ltimo, basta acresentar ``TranformerMixin`` como uma classe. Al√©m disso, se adicionar o ``BaseEstimator`` como uma classe, voc√™ ter√° acesso a todos os m√©todos de estimadores do Sklearn, como ``get_params()`` e ``set_params()``. Isso √© √∫til para definir os hiperpar√¢metros do seu transformador.\n",
    "\n",
    "Por exemplo, inv√©s de desenvolver o c√≥digo anterior do **KNNImputer** de forma manual, voc√™ pode usar o transformador do Sklearn para automatizar o processo e tornar ele reutiliz√°vel. O c√≥digo abaixo mostra como fazer isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e16fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedAttributeOther(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col, k_candidates=range(1,11), cv=4):\n",
    "        \"\"\"\n",
    "        Inicializa o imputador customizado.\n",
    "\n",
    "        Par√¢metros:\n",
    "        -----------\n",
    "        target_col : str\n",
    "            Nome da coluna alvo que cont√©m valores faltantes a serem imputados.\n",
    "        k_candidates : iterable, default=range(1,11)\n",
    "            Lista ou intervalo com os valores de 'k' vizinhos para testar no KNNImputer.\n",
    "        cv : int, default=4\n",
    "            N√∫mero de folds para valida√ß√£o cruzada na busca do melhor 'k'.\n",
    "        \"\"\"\n",
    "        self.target_col = target_col\n",
    "        self.k_candidates = list(k_candidates)\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Ajusta o imputador aos dados, encontrando o melhor valor de 'k' via valida√ß√£o cruzada,\n",
    "        escalonando os dados e treinando o imputador final.\n",
    "\n",
    "        Par√¢metros:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Dataset completo contendo a coluna alvo e demais vari√°veis num√©ricas.\n",
    "        y : Ignorado (compatibilidade com sklearn)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        self : objeto ajustado\n",
    "        \"\"\"\n",
    "        # Seleciona as colunas num√©ricas do dataset\n",
    "        self.numeric_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        # Verifica se a coluna alvo √© num√©rica e est√° no dataset\n",
    "        if self.target_col not in self.numeric_cols_:\n",
    "            raise ValueError(f\"Coluna alvo '{self.target_col}' deve ser num√©rica e estar no DataFrame\")\n",
    "\n",
    "        # Usa apenas as linhas onde a coluna alvo n√£o est√° faltando para treinamento/valida√ß√£o\n",
    "        train_data = X.dropna(subset=[self.target_col])\n",
    "        \n",
    "        # Separa features (num√©ricas, menos a target) e target\n",
    "        X_train = train_data[self.numeric_cols_].drop(columns=[self.target_col])\n",
    "        y_train = train_data[self.target_col]\n",
    "\n",
    "        # Escalona os dados num√©ricos para melhorar desempenho do KNN\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        best_k = None\n",
    "        best_score = np.inf  # Inicializa com infinito para buscar m√≠nimo RMSE\n",
    "\n",
    "        # Configura valida√ß√£o cruzada com embaralhamento para robustez\n",
    "        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n",
    "\n",
    "        # Para cada candidato a 'k', avalia o desempenho m√©dio com CV\n",
    "        for k in self.k_candidates:\n",
    "            scores = []\n",
    "            for train_idx, val_idx in kf.split(X_train_scaled):\n",
    "                # Dados treino e valida√ß√£o do fold\n",
    "                X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "                # Cria e treina o imputador KNN para o fold com k vizinhos\n",
    "                imputer = KNNImputer(n_neighbors=k)\n",
    "                X_tr_imputed = imputer.fit_transform(np.c_[X_tr, y_tr.values.reshape(-1,1)])\n",
    "\n",
    "                # Imputa valores no conjunto de valida√ß√£o\n",
    "                X_val_imputed = imputer.transform(np.c_[X_val, y_val.values.reshape(-1,1)])\n",
    "\n",
    "                # Pega s√≥ a coluna alvo imputada\n",
    "                y_val_pred = X_val_imputed[:, -1]\n",
    "\n",
    "                # Calcula RMSE do fold (erro entre valor original e imputado)\n",
    "                score = np.sqrt(np.mean((y_val.values - y_val_pred)**2))\n",
    "                scores.append(score)\n",
    "\n",
    "            # M√©dia dos scores dos folds para este k\n",
    "            mean_score = np.mean(scores)\n",
    "\n",
    "            # Atualiza melhor k se melhor resultado\n",
    "            if mean_score < best_score:\n",
    "                best_score = mean_score\n",
    "                best_k = k\n",
    "\n",
    "        # Armazena o melhor k encontrado\n",
    "        self.best_k_ = best_k\n",
    "\n",
    "        # Salva o scaler (para uso na transforma√ß√£o)\n",
    "        self.scaler_ = scaler\n",
    "\n",
    "        # Cria imputador final com melhor k encontrado\n",
    "        self.imputer_ = KNNImputer(n_neighbors=best_k)\n",
    "\n",
    "        # Treina o imputador com o dataset completo, escalonado\n",
    "        X_full = X[self.numeric_cols_].drop(columns=[self.target_col])\n",
    "        y_full = X[self.target_col]\n",
    "        X_scaled_full = scaler.transform(X_full)\n",
    "\n",
    "        self.imputer_.fit(np.c_[X_scaled_full, y_full.values.reshape(-1,1)])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Aplica a imputa√ß√£o no dataset X fornecido, substituindo os valores faltantes\n",
    "        da coluna alvo pelos valores imputados pelo KNNImputer treinado.\n",
    "\n",
    "        Par√¢metros:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Dataset para transformar/imputar.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        X_new : pd.DataFrame\n",
    "            Dataset com a coluna alvo imputada.\n",
    "        \"\"\"\n",
    "        # Copia as colunas num√©ricas do dataset\n",
    "        X_num = X[self.numeric_cols_].copy()\n",
    "\n",
    "        # Escalona as colunas, exceto a coluna alvo\n",
    "        X_num_scaled = self.scaler_.transform(X_num.drop(columns=[self.target_col]))\n",
    "\n",
    "        # Realiza imputa√ß√£o concatenando as features com a coluna alvo (que pode ter NAs)\n",
    "        imputed = self.imputer_.transform(np.c_[X_num_scaled, X_num[self.target_col].values.reshape(-1,1)])\n",
    "\n",
    "        # Atualiza a coluna alvo com os valores imputados\n",
    "        X_num[self.target_col] = imputed[:, -1]\n",
    "\n",
    "        # Cria uma c√≥pia do DataFrame original para n√£o alterar inplace\n",
    "        X_new = X.copy()\n",
    "\n",
    "        # Atualiza as colunas num√©ricas com os valores (incluindo a coluna alvo imputada)\n",
    "        X_new[self.numeric_cols_] = X_num\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Combina os passos fit e transform para facilitar uso.\n",
    "\n",
    "        Par√¢metros:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Dataset para ajustar e transformar.\n",
    "        y : Ignorado (compatibilidade sklearn)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        X_new : pd.DataFrame\n",
    "            Dataset com coluna alvo imputada ap√≥s ajuste.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9ad38",
   "metadata": {},
   "source": [
    "Exemplo pr√°tico de aplica√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365bb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que a classe CombinedAttributeOther j√° foi definida/importada aqui\n",
    "\n",
    "# Criando um DataFrame de exemplo com valores faltantes na coluna alvo 'target'\n",
    "data = {\n",
    "    'feature1': [1.0, 2.0, 3.0, 4.0, 5.0, np.nan, 7.0],\n",
    "    'feature2': [10, 9, 8, 7, 6, 5, 4],\n",
    "    'target': [100, 200, np.nan, 400, 500, 600, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset original:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedfe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o imputador para a coluna alvo 'target'\n",
    "imputer = CombinedAttributeOther(target_col='target')\n",
    "\n",
    "# 1. Usando fit(): ajusta o imputador nos dados\n",
    "imputer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Usando transform(): aplica a imputa√ß√£o (necess√°rio j√° ter feito fit)\n",
    "df_imputed = imputer.transform(df)\n",
    "\n",
    "print(\"\\nDataset ap√≥s transform (imputa√ß√£o aplicada):\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o imputador para a coluna alvo 'target'\n",
    "imputer = CombinedAttributeOther(target_col='feature1')\n",
    "\n",
    "# 3. Usando fit_transform(): faz o ajuste e j√° retorna o dataset imputado\n",
    "df_imputed_2 = imputer.fit_transform(df)\n",
    "\n",
    "print(\"\\nDataset ap√≥s fit_transform (ajuste + imputa√ß√£o):\")\n",
    "print(df_imputed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.best_k_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4258466",
   "metadata": {},
   "source": [
    "### ESCALONAMENTO DOS DADOS\n",
    "\n",
    "O escalonamento de dados √© uma etapa crucial no pr√©-processamento, especialmente quando se utiliza algoritmos baseados em dist√¢ncia, como o KNN. O escalonamento garante que todas as vari√°veis contribuam igualmente para a dist√¢ncia calculada, evitando que vari√°veis com escalas maiores dominem a an√°lise. Para al√©m do KNN, o escalonamento √© importante para muitos algoritmos de aprendizado de m√°quina, como regress√£o log√≠stica, SVM e redes neurais. O escalonamento pode ser feito de v√°rias maneiras, incluindo:\n",
    "- **Min-Max Scaling:** Transforma os dados para um intervalo espec√≠fico, geralmente [0, 1].\n",
    "- **Padroniza√ß√£o (Z-score):** Transforma os dados para que tenham m√©dia 0 e desvio padr√£o 1.\n",
    "- **Robust Scaling:** Usa a mediana e o intervalo interquartil para escalonar os dados, tornando-o robusto a outliers.\n",
    "- **Log Transformation:** Aplica a transforma√ß√£o logar√≠tmica para lidar com distribui√ß√µes assim√©tricas.\n",
    "- **Quantile Transformation:** Transforma os dados para uma distribui√ß√£o uniforme ou normal, √∫til para lidar com distribui√ß√µes n√£o gaussianas.\n",
    "- **E entre outros...**\n",
    "\n",
    "Entretanto, em Machine Learning, o escalonamento √© uma etapa importante, mas n√£o √© sempre necess√°rio. Por exemplo, algoritmos baseados em √°rvores (como Decision Trees e Random Forests) n√£o s√£o sens√≠veis √† escala dos dados, ent√£o o escalonamento pode n√£o ser necess√°rio. No entanto, para algoritmos que dependem de dist√¢ncias, como KNN e SVM, o escalonamento √© essencial para garantir um desempenho adequado. E mais, como em todas as etapas do pr√©-processamento, o escalonamento deve ser feito com cuidado, considerando o contexto dos dados e o algoritmo que ser√° utilizado. O uso de t√©cnicas de escalonamento pode melhorar a performance do modelo e garantir que todas as vari√°veis sejam tratadas de forma justa. Como em todas as transforma√ß√µes, √© importante ajustar os escalonamentos apenas aos dados de treinamento, n√£o ao conjunto de dados completo (incluindo o conjunto de testes). S√≥ ent√£o, o escalonamento deve ser aplicado ao conjunto de teste usando os par√¢metros calculados no conjunto de treinamento. Isso garante que o modelo seja avaliado de forma justa e evita vazamento de dados.\n",
    "\n",
    "### Demonstra√ß√£o Matem√°tica dos M√©todos de Escalonamento\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Padroniza√ß√£o (StandardScaler)\n",
    "\n",
    "A padroniza√ß√£o transforma os dados para que tenham m√©dia zero e desvio padr√£o 1, aplicando a f√≥rmula:\n",
    "\n",
    "$\n",
    "x' = \\dfrac{x - \\mu}{\\sigma}\n",
    "$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $x$ √© o valor original da amostra,\n",
    "- $\\mu$ √© a m√©dia dos dados do conjunto (geralmente calculada no conjunto de treinamento),\n",
    "- $\\sigma$ √© o desvio padr√£o dos dados do conjunto.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. MinMaxScaler\n",
    "\n",
    "O MinMaxScaler reescala os dados para um intervalo espec√≠fico $[a, b]$. A f√≥rmula geral √©:\n",
    "\n",
    "$\n",
    "x' = a + \\dfrac{(x - x_{\\min})(b - a)}{x_{\\max} - x_{\\min}}\n",
    "$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $x$ √© o valor original da amostra,\n",
    "- $x_{\\min}$ e $x_{\\max}$ s√£o, respectivamente, o valor m√≠nimo e m√°ximo dos dados do conjunto,\n",
    "- $a$ e $b$ s√£o os limites inferior e superior do intervalo desejado.\n",
    "\n",
    "---\n",
    "\n",
    "##### Caso 1: MinMaxScaler para o intervalo $[0, 1]$\n",
    "\n",
    "Aqui, $a = 0$ e $b = 1$, ent√£o a f√≥rmula simplifica para:\n",
    "\n",
    "$\n",
    "x' = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "##### Caso 2: MinMaxScaler para o intervalo $[1, 1]$\n",
    "\n",
    "Note que $[1, 1]$ √© um intervalo degenerado, ou seja, os limites inferior e superior s√£o iguais. Nesse caso, a f√≥rmula se torna:\n",
    "\n",
    "$\n",
    "x' = 1 + \\dfrac{(x - x_{\\min})(1 - 1)}{x_{\\max} - x_{\\min}} = 1 + 0 = 1\n",
    "$\n",
    "\n",
    "Ou seja, **todos os valores s√£o mapeados para 1**, j√° que n√£o h√° varia√ß√£o no intervalo.\n",
    "\n",
    "---\n",
    "\n",
    "# Resumo\n",
    "\n",
    "| T√©cnica         | F√≥rmula                                               | Intervalo padr√£o         |\n",
    "|-----------------|------------------------------------------------------|-------------------------|\n",
    "| Padroniza√ß√£o    | $x' = \\dfrac{x - \\mu}{\\sigma}$                      | M√©dia 0, desvio padr√£o 1 |\n",
    "| MinMaxScaler    | $x' = a + \\dfrac{(x - x_{\\min})(b - a)}{x_{\\max} - x_{\\min}}$ | Vari√°vel, ex: [0, 1]    |\n",
    "| MinMaxScaler [0,1] | $x' = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$ | [0, 1]                  |\n",
    "| MinMaxScaler [1,1] | $x' = 1$                                          | Degenerado, tudo igual 1 |\n",
    "\n",
    "Exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff98a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé≤ Define uma semente (seed) para o gerador de n√∫meros aleat√≥rios\n",
    "# Isso garante que os resultados sejam reproduz√≠veis, ou seja, \n",
    "# toda vez que rodar o c√≥digo, os n√∫meros gerados ser√£o os mesmos\n",
    "np.random.seed(42)\n",
    "\n",
    "# üéØ Cria um conjunto de dados aleat√≥rios seguindo uma distribui√ß√£o normal (Gaussiana)\n",
    "\n",
    "data = np.random.normal(\n",
    "    loc=20,      # üëâ loc √© a M√âDIA da distribui√ß√£o (nesse caso, 20)\n",
    "    scale=5,     # üëâ scale √© o DESVIO PADR√ÉO (a dispers√£o dos dados, aqui √© 5)\n",
    "    size=1000    # üëâ size define a quantidade de valores que ser√£o gerados (1000 valores)\n",
    ").reshape(-1, 1) # üëâ reshape(-1, 1) transforma o array de uma dimens√£o (1D) \n",
    "                 # em um array bidimensional (2D) com 1000 linhas e 1 coluna.\n",
    "                 # Isso √© √∫til para compatibilidade com modelos e fun√ß√µes \n",
    "                 # do scikit-learn, que geralmente trabalham com matrizes (2D).\n",
    "\n",
    "# üîç Resultado:\n",
    "# 'data' √© um array de forma (1000, 1) contendo 1000 valores simulados\n",
    "# que seguem uma distribui√ß√£o normal com m√©dia 20 e desvio padr√£o 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Padroniza√ß√£o padr√£o com StandardScaler (m√©dia = 0, vari√¢ncia = 1)\n",
    "\n",
    "# üîß Cria um objeto do StandardScaler\n",
    "# Esse scaler transforma os dados para que tenham:\n",
    "# ‚Üí m√©dia = 0\n",
    "# ‚Üí desvio padr√£o = 1 (vari√¢ncia = 1)\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# üöÄ Ajusta o scaler aos dados (fit) e transforma (transform) de uma vez\n",
    "# ‚Üí Calcula a m√©dia e o desvio padr√£o dos dados\n",
    "# ‚Üí Retorna um array padronizado: (x - m√©dia) / desvio_padr√£o\n",
    "data_standard = scaler_standard.fit_transform(data)\n",
    "\n",
    "\n",
    "# 2Ô∏è‚É£ Padroniza√ß√£o customizada: m√©dia = 5, vari√¢ncia = 2\n",
    "\n",
    "# üëâ Aqui pegamos os dados j√° padronizados (m√©dia = 0, desvio = 1)\n",
    "# e fazemos uma transforma√ß√£o linear para alterar a escala:\n",
    "\n",
    "# Multiplicamos pelos novo desvio padr√£o:\n",
    "# ‚Üí Desvio padr√£o = ‚àö2, pois vari√¢ncia = 2 (vari√¢ncia = desvio¬≤)\n",
    "# E somamos a nova m√©dia = 5\n",
    "data_standard_custom = data_standard * np.sqrt(2) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28936016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Cria um objeto MinMaxScaler com intervalo padr√£o (0, 1)\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# üöÄ Ajusta (fit) e transforma (transform) os dados\n",
    "# ‚Üí Escala os dados para que todos fiquem no intervalo de 0 a 1\n",
    "data_minmax = scaler_minmax.fit_transform(data)\n",
    "\n",
    "# üîß Cria um MinMaxScaler que ajusta os dados para o intervalo (0, 10)\n",
    "scaler_minmax_custom = MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "# üöÄ Ajusta e transforma os dados para esse novo intervalo\n",
    "data_minmax_custom = scaler_minmax_custom.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria figura com 2 linhas x 2 colunas de subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 9))                    # figsize define o tamanho em polegadas\n",
    "fig.suptitle('Compara√ß√£o dos M√©todos de Escalonamento de Dados', fontsize=16, weight='bold')\n",
    "\n",
    "# Subplot 1: histograma da padroniza√ß√£o padr√£o\n",
    "axs[0, 0].hist(data_standard, bins=40,                           # N√∫mero de barras no histograma\n",
    "               color='steelblue', edgecolor='black', alpha=0.8)  # Cores e transpar√™ncia\n",
    "axs[0, 0].set_title('Padroniza√ß√£o Padr√£o\\n(m√©dia=0, vari√¢ncia=1)', fontsize=12, weight='bold')\n",
    "axs[0, 0].set_xlabel('Valor padronizado')                        # Label do eixo X\n",
    "axs[0, 0].set_ylabel('Frequ√™ncia')                               # Label do eixo Y\n",
    "axs[0, 0].grid(True)                                             # Ativa grade\n",
    "\n",
    "# Subplot 2: histograma da padroniza√ß√£o customizada\n",
    "axs[0, 1].hist(data_standard_custom, bins=40,\n",
    "               color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "axs[0, 1].set_title('Padroniza√ß√£o Customizada\\n(m√©dia=5, vari√¢ncia=2)', fontsize=12, weight='bold')\n",
    "axs[0, 1].set_xlabel('Valor padronizado customizado')\n",
    "axs[0, 1].set_ylabel('Frequ√™ncia')\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Subplot 3: histograma do MinMaxScaler padr√£o (0 a 1)\n",
    "axs[1, 0].hist(data_minmax, bins=40,\n",
    "               color='tomato', edgecolor='black', alpha=0.8)\n",
    "axs[1, 0].set_title('MinMaxScaler Padr√£o\\n(Intervalo: 0 a 1)', fontsize=12, weight='bold')\n",
    "axs[1, 0].set_xlabel('Valor escalado (0‚Äì1)')\n",
    "axs[1, 0].set_ylabel('Frequ√™ncia')\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# Subplot 4: histograma do MinMaxScaler customizado (0 a 10)\n",
    "axs[1, 1].hist(data_minmax_custom, bins=40,\n",
    "               color='darkorange', edgecolor='black', alpha=0.8)\n",
    "axs[1, 1].set_title('MinMaxScaler Customizado\\n(Intervalo: 0 a 10)', fontsize=12, weight='bold')\n",
    "axs[1, 1].set_xlabel('Valor escalado (0‚Äì10)')\n",
    "axs[1, 1].set_ylabel('Frequ√™ncia')\n",
    "axs[1, 1].grid(True)\n",
    "\n",
    "# Ajusta layout para evitar sobreposi√ß√£o de elementos\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df244449",
   "metadata": {},
   "source": [
    "## **Feature Engeneering**\n",
    "\n",
    "Como diz o ditado: entra lixo, sai lixo. Seu sistema de s√≥ ser√° capaz de aprender se os dados de treinamento tiverem caracter√≠sticas relevantes o suficientes e poucas caracter√≠sticas irrelevantes. Uma parte imprescind√≠vel do sucesso de um projeto de ML √© criar um bom conjunto de caracter√≠sticas para o treinamento, processo chamado de *feature engeneering* (ou engenharia de features) que envolve os seguintes passos:\n",
    "\n",
    "- *Sele√ß√£o de caracter√≠sticas* (selecionar as caracter√≠sticas mais √∫teis para treinamento entre as caracter√≠sticas existentes)\n",
    "- *Extra√ß√£o de caracter√≠sticas* (combinar caracter√≠sticas existentes a fim de obter as mais √∫teis)\n",
    "- Cria√ß√£o de novas caracter√≠sticas ao coletar dados novos.\n",
    "\n",
    "At√© o momento, lidamos apenas com atributos num√©ricos, mas agora analisaremos os atributosde texto. Neste conjunto de dados, existe somente uma vari√°vel: o atributo ``ocean_proximity``, vamos analisar ele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Calcula as porcentagens de cada categoria na coluna 'ocean_proximity'\n",
    "ocean_proximity_percent = round((housing_imputed.ocean_proximity.value_counts() / len(housing_imputed)) * 100, 2)\n",
    "\n",
    "# Cria dataframe\n",
    "df_percent = pd.DataFrame({\n",
    "    'Localiza√ß√£o': ocean_proximity_percent.index,\n",
    "    'Porcentagem (%)': ocean_proximity_percent.values\n",
    "})\n",
    "\n",
    "# üìú Gera a string em formato Markdown para exibir como tabela\n",
    "markdown_table = \"### Distribui√ß√£o das Localiza√ß√µes (`ocean_proximity`)\\n\\n\"\n",
    "markdown_table += df_percent.to_markdown(index=False)\n",
    "\n",
    "# üî• Exibe como markdown\n",
    "display(Markdown(markdown_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45c526",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02373efd",
   "metadata": {},
   "source": [
    "A maioria dos algoritmos de ML prefere trabalhar com n√∫meros, para corrigir esse problema, uma solu√ß√£o comum √© criar um atributo bin√°rio por categoria, isso se chama *codifica√ß√£o one-hot* [*one-hot enconding* ou ainda *codifica√ß√£o distribu√≠da*], porque apenas um atributo ser√° igual a 1, enquanto os outros ser√£o 0. Os atributos novos se chamam atributos falsos [*dummy*]. ``sklearn`` fornece uma uma classe ``OneHotEncoder()`` para converter valores categ√≥ricos em valores one-hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e189c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_imputed[['ocean_proximity']])\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e0977",
   "metadata": {},
   "source": [
    "### Combina√ß√µes de Atributos\n",
    "\n",
    "Uma das √∫ltimas que voc√™ pode querer fazer antes de preparar os dados para os algoritmos de ML √© testar diferentes combina√ß√µes de atributos para gerar novos atributos. Por exemplo:\n",
    "- O n√∫mero total de c√¥modos em uma determinada regi√£o n√£o servir√° de nadda se n√£o souber quantas fam√≠lias vivem nessa regi√£o.\n",
    "- Do mesmo modo, o n√∫mero total de quartos propriamente dito n√£o ajuda muito: voc√™ provavelmente vai querer compr√°-lo com o n√∫mero de c√¥modos.\n",
    "- E ao que tudo indica, a popula√ß√£o por domic√≠lio tamb√©m √© uma combina√ß√£o de taributos interressante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f31de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Feature Engineering: cria√ß√£o de novas vari√°veis\n",
    "\n",
    "# üîß rooms_per_household -> n√∫mero m√©dio de quartos por domic√≠lio\n",
    "housing_imputed['rooms_per_household'] = housing_imputed['total_rooms'] / housing_imputed['households']\n",
    "\n",
    "# üîß bedrooms_per_room -> propor√ß√£o de quartos que s√£o dormit√≥rios (mede a densidade de dormit√≥rios)\n",
    "housing_imputed['bedrooms_per_room'] = housing_imputed['total_bedrooms'] / housing_imputed['total_rooms']\n",
    "\n",
    "# üîß population_for_household -> n√∫mero m√©dio de pessoas por domic√≠lio\n",
    "housing_imputed['population_for_household'] = housing_imputed['population'] / housing_imputed['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c868448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Seleciona apenas as novas vari√°veis criadas\n",
    "df_new_features = housing_imputed[['rooms_per_household', 'bedrooms_per_room', 'population_for_household']].head()\n",
    "\n",
    "# üìú Cria a string da tabela em Markdown\n",
    "markdown_table = \"### üîç Novas Vari√°veis Criadas (`Feature Engineering`)\\n\\n\"\n",
    "markdown_table += df_new_features.to_markdown(index=False)\n",
    "\n",
    "# üìä Exibe como Markdown no output\n",
    "display(Markdown(markdown_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd607711",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing_imputed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada56aa",
   "metadata": {},
   "source": [
    "### Sele√ß√£o de Vari√°veis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a99356",
   "metadata": {},
   "source": [
    "#### Visualizando Dados Geogr√°ficos\n",
    "\n",
    "Uma vez que temos informa√ß√µes geogr√°ficas (latitude e longutude), √© uma boa ideia criar um diagrama de dispers√£o para visualizar os dados de todas as regi√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem de um gr√°fico de dispers√£o (scatter plot) usando DataFrame 'housing_imputed'\n",
    "# Esse gr√°fico representa as casas de acordo com sua longitude e latitude, com v√°rias codifica√ß√µes visuais.\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Define o tamanho da figura em polegadas (mais espa√ßo, melhor visualiza√ß√£o)\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    housing_imputed['longitude'],             # Eixo x: longitude das casas\n",
    "    housing_imputed['latitude'],              # Eixo y: latitude das casas\n",
    "    alpha=0.5,                                # Transpar√™ncia dos pontos, de 0 (invis√≠vel) a 1 (opaco)\n",
    "    s=housing_imputed['population'] / 100,    # Tamanho dos pontos proporcional √† popula√ß√£o local\n",
    "    c=housing_imputed['median_house_value'],  # Cor dos pontos de acordo com o valor mediano das casas\n",
    "    cmap='viridis',                           # Mapa de cores mais moderno e percept√≠vel (substituindo 'jet')\n",
    "    edgecolor='k',                            # Adiciona contorno preto aos pontos\n",
    "    linewidth=0.5                             # Define a espessura da linha do contorno\n",
    ")\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=14)          # R√≥tulo do eixo x com tamanho de fonte maior\n",
    "plt.ylabel('Latitude', fontsize=14)           # R√≥tulo do eixo y\n",
    "\n",
    "plt.title('Distribui√ß√£o Geogr√°fica das Casas na Calif√≥rnia', fontsize=16)  # T√≠tulo do gr√°fico\n",
    "\n",
    "cbar = plt.colorbar(scatter)                  # Adiciona barra de cores ao lado\n",
    "cbar.set_label('Valor Mediano das Casas', fontsize=12)  # R√≥tulo da barra de cores\n",
    "\n",
    "plt.legend(['Popula√ß√£o (escala do tamanho dos pontos)'], fontsize=12)  # Legenda explicando o tamanho\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)     # Adiciona grade com linhas tracejadas e leve transpar√™ncia\n",
    "\n",
    "plt.tight_layout()                            # Ajusta automaticamente o layout para n√£o cortar elementos\n",
    "\n",
    "plt.show()                                    # Exibe o gr√°fico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9737c4",
   "metadata": {},
   "source": [
    "O raio de cada c√≠rculo representa a popula√ß√£o (op√ß√£o ``s``) e a cor representa o pre√ßo (op√ß√£o ``c``). Usamos um mapa de cores predefinido (op√ß√£o ``cmap``) chamado ``virids``, que varia de roxo (valores baixos) para amarelo (valores altos). Esta imagem informa que os pre√ßos dos im√≥veis est√£o muito relacionados √† localiza√ß√£o (por exemplo proximidade do mar) e √† densidade populacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0fd1e",
   "metadata": {},
   "source": [
    "#### Buscando **Correla√ß√µes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f1e07",
   "metadata": {},
   "source": [
    "##### Correla√ß√£o de Pearson\n",
    "\n",
    "A correla√ß√£o de Pearson √© frequentemente aplicada de forma direta em an√°lises explorat√≥rias, como visto no livro de Aur√©lien G√©ron (*Hands-On Machine Learning*), devido √† sua simplicidade e utilidade para rapidamente identificar padr√µes de depend√™ncia linear entre vari√°veis.\n",
    "\n",
    "No entanto, √© importante destacar que, para uma interpreta√ß√£o estat√≠stica rigorosa e inferencial da correla√ß√£o, recomenda-se verificar os seguintes pressupostos: linearidade, normalidade, homocedasticidade, escala intervalar e aus√™ncia de outliers.\n",
    "\n",
    "1Ô∏è‚É£ O objetivo final √© predi√ß√£o, n√£o infer√™ncia causal.\n",
    "\n",
    "Logo, o foco n√£o √© fazer testes estat√≠sticos para validar hip√≥teses ou explicar rela√ß√µes entre vari√°veis com validade inferencial, mas **melhorar a acur√°cia ou outra m√©trica do modelo preditivo**.\n",
    "\n",
    "‚û°Ô∏è Nestes casos, **N√ÉO √© necess√°rio exigir os pressupostos cl√°ssicos da correla√ß√£o de Pearson**, como linearidade ou normalidade.\n",
    "\n",
    "‚û°Ô∏è A correla√ß√£o pode ser usada como uma **heur√≠stica r√°pida** para identificar redund√¢ncias ou depend√™ncias fortes, e guiar a escolha de features.\n",
    "\n",
    "##### üß† **L√≥gica da Correla√ß√£o de Pearson**\n",
    "\n",
    "A correla√ß√£o de Pearson mede o grau de **associa√ß√£o linear** entre duas vari√°veis num√©ricas.\n",
    "\n",
    "A f√≥rmula √©:\n",
    "\n",
    "$\n",
    "r = \\dfrac{\\sum (X_i - \\bar{X}) (Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum (Y_i - \\bar{Y})^2}}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $X_i$ e $Y_i$ s√£o os valores das vari√°veis X e Y.\n",
    "- $\\bar{X}$ e $\\bar{Y}$ s√£o as m√©dias de X e Y.\n",
    "- $r$ √© o coeficiente de correla√ß√£o de Pearson, que varia entre -1 e 1.\n",
    "\n",
    "**Interpreta√ß√£o:**\n",
    "\n",
    "- $r = 1$: Correla√ß√£o linear positiva perfeita.\n",
    "- $r = -1$: Correla√ß√£o linear negativa perfeita.\n",
    "- $r = 0$: Aus√™ncia de correla√ß√£o linear (mas n√£o implica independ√™ncia total).\n",
    "\n",
    "Ele √©, basicamente, a **covari√¢ncia padronizada** entre duas vari√°veis. A padroniza√ß√£o ocorre ao dividir pela multiplica√ß√£o dos desvios padr√£o, permitindo que o coeficiente sempre esteja na escala de -1 a 1.\n",
    "\n",
    "---\n",
    "\n",
    "##### Correla√ß√£o de Spearman\n",
    "\n",
    "A correla√ß√£o de Spearman √© uma medida n√£o-param√©trica que avalia a **for√ßa e a dire√ß√£o da associa√ß√£o monot√¥nica** entre duas vari√°veis, isto √©, verifica se √† medida que uma vari√°vel aumenta, a outra tende a aumentar ou diminuir, sem exigir que essa rela√ß√£o seja linear.\n",
    "\n",
    "√â especialmente √∫til quando:\n",
    "\n",
    "- Os dados n√£o seguem uma distribui√ß√£o normal.\n",
    "- H√° presen√ßa de outliers.\n",
    "- As rela√ß√µes s√£o n√£o-lineares, mas ainda monot√¥nicas.\n",
    "\n",
    "##### üß† **L√≥gica da Correla√ß√£o de Spearman**\n",
    "\n",
    "O primeiro passo √© transformar os dados em **ranks** (ordens). Cada valor de X e Y √© substitu√≠do por sua posi√ß√£o na ordena√ß√£o dos dados.\n",
    "\n",
    "A f√≥rmula da correla√ß√£o de Spearman, quando n√£o h√° empates, √©:\n",
    "\n",
    "$\n",
    "r_s = 1 - \\dfrac{6 \\sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}\n",
    "$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $d_i$ = diferen√ßa entre os ranks de cada par de observa√ß√µes $(X_i, Y_i)$.\n",
    "- $n$ = n√∫mero de observa√ß√µes.\n",
    "- $r_s$ = coeficiente de correla√ß√£o de Spearman.\n",
    "\n",
    "Se houver empates nos dados, usa-se a mesma l√≥gica da correla√ß√£o de Pearson, mas aplicada aos ranks em vez dos valores originais.\n",
    "\n",
    "**Interpreta√ß√£o:**\n",
    "\n",
    "- $r_s = 1$: Rela√ß√£o monot√¥nica crescente perfeita.\n",
    "- $r_s = -1$: Rela√ß√£o monot√¥nica decrescente perfeita.\n",
    "- $r_s = 0$: Aus√™ncia de rela√ß√£o monot√¥nica.\n",
    "\n",
    "‚û°Ô∏è Assim como a correla√ß√£o de Pearson mede associa√ß√µes lineares, a de Spearman amplia essa an√°lise para rela√ß√µes monot√¥nicas, oferecendo maior robustez em cen√°rios mais realistas do mundo dos dados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13574360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from correlation_analyzer import CorrelationAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00428b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = CorrelationAnalyzer(housing_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.correlation(target_col='median_house_value', method='pearson', sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.correlation(target_col='median_house_value', method='spearman', sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4825203",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot_correlation_matrix(figsize= (10, 10), method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot_correlation_matrix(figsize= (10, 10), method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7046525",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot_scatter_matrix(figsize=(12, 12), variables=['median_house_value', 'median_income', 'total_rooms',\n",
    "                                                    'housing_median_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa04667",
   "metadata": {},
   "source": [
    "## **Criando o Conjunto de Treinamento e Teste**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7db1a",
   "metadata": {},
   "source": [
    "Pode parecer estranho separar voluntariamente uma parte dos dados neste est√°gio. Afinal, voc√™ apenas deu uma olhada r√°pida nos dados, e certamente deveria aprender muito mais sobre eles antes de decidir quais algoritmos utilizar, certo? Isso √© verdade, mas seu c√©rebro √© um sistema incr√≠vel de detec√ß√£o de padr√µes, o que tamb√©m significa que ele √© altamente propenso ao **overfitting**: se voc√™ olhar para o conjunto de teste, pode acabar encontrando algum padr√£o aparentemente interessante nos dados de teste que o leva a escolher um tipo espec√≠fico de modelo de machine learning.\n",
    "\n",
    "Quando voc√™ estima o erro de generaliza√ß√£o usando o conjunto de teste, essa estimativa ser√° **otimista demais**, e voc√™ pode acabar lan√ßando um sistema que n√£o ter√° um desempenho t√£o bom quanto o esperado. Isso √© conhecido como **data snooping bias** (vi√©s de bisbilhotagem dos dados).\n",
    "\n",
    "A cria√ß√£o de um conjunto de teste √©, teoricamente, simples: selecione algumas inst√¢ncias aleatoriamente ‚Äî tipicamente **20% do conjunto de dados** (ou menos, se seu dataset for muito grande) ‚Äî e separe-as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    \"\"\"\n",
    "    Divide os dados em conjuntos de treinamento e teste de forma aleat√≥ria.\n",
    "\n",
    "    -------------------------------\n",
    "    üîß Par√¢metros:\n",
    "    - data: DataFrame contendo os dados que ser√£o divididos.\n",
    "      ‚û°Ô∏è √â o dataset completo que voc√™ deseja particionar.\n",
    "\n",
    "    - test_ratio: float\n",
    "      ‚û°Ô∏è Propor√ß√£o dos dados que ser√£o separados para o conjunto de teste.\n",
    "      ‚û°Ô∏è Exemplo: se test_ratio = 0.2, ent√£o 20% dos dados ser√£o usados para teste,\n",
    "         e 80% para treino.\n",
    "\n",
    "    -------------------------------\n",
    "    üîô Retorna:\n",
    "    - train_data: DataFrame com os dados de treinamento.\n",
    "    - test_data: DataFrame com os dados de teste.\n",
    "\n",
    "    -------------------------------\n",
    "    üß† L√≥gica do algoritmo:\n",
    "\n",
    "    1Ô∏è‚É£ Cria uma sequ√™ncia de √≠ndices embaralhados dos dados.\n",
    "    2Ô∏è‚É£ Calcula o tamanho do conjunto de teste com base no test_ratio.\n",
    "    3Ô∏è‚É£ Separa os √≠ndices do conjunto de teste e do conjunto de treino.\n",
    "    4Ô∏è‚É£ Retorna os subconjuntos correspondentes de treino e teste.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gera uma permuta√ß√£o aleat√≥ria dos √≠ndices do DataFrame.\n",
    "    # Isso garante que a divis√£o seja aleat√≥ria a cada execu√ß√£o.\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "\n",
    "    # Calcula o n√∫mero de amostras que ir√£o compor o conjunto de teste.\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "\n",
    "    # Seleciona os primeiros 'test_set_size' √≠ndices para o conjunto de teste.\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "\n",
    "    # O restante dos √≠ndices ser√° usado para o conjunto de treinamento.\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "\n",
    "    # Retorna os subconjuntos de treino e teste, utilizando os √≠ndices gerados.\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67100a64",
   "metadata": {},
   "source": [
    "Podemos usar uma fun√ß√£o como essa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aa1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing_imputed, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "### üîç Conjunto de Treinamento (`train_set`)\n",
    "\n",
    "{train_set.head().to_markdown(index=False)}\n",
    "\n",
    "### üîç Conjunto de Teste (`test_set`)\n",
    "\n",
    "{test_set.head().to_markdown(index=False)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a44d71",
   "metadata": {},
   "source": [
    "Isso funciona, mas h√° um defeito: ao executar novamente, ser√° gerado um conjunto diferente de teste e treino! Ao longo do tempo, voc√™ veria todo o conjunto de dados ao repetir esse processo, entretanto o **Scikit-Learn** oferece algumas fun√ß√µes para dividir conjuntos de dados em m√∫ltiplos subconjuntos de diferentes formas. \n",
    "\n",
    "A fun√ß√£o mais simples √© a **`train_test_split()`**, que realiza praticamente a mesma opera√ß√£o que a fun√ß√£o **`split_train_test()`** que definimos anteriormente, por√©m com alguns recursos adicionais importantes:\n",
    "\n",
    "1Ô∏è‚É£ Primeiramente, h√° o par√¢metro **`random_state`**, que permite definir a semente do gerador aleat√≥rio. Isso garante que a divis√£o dos dados seja **reprodut√≠vel**, ou seja, sempre gere os mesmos resultados se a mesma semente for usada.\n",
    "\n",
    "2Ô∏è‚É£ Al√©m disso, √© poss√≠vel passar **m√∫ltiplos conjuntos de dados com o mesmo n√∫mero de linhas**, e a fun√ß√£o ir√° dividi-los utilizando os **mesmos √≠ndices**. \n",
    "\n",
    "‚û°Ô∏è Isso √© extremamente √∫til, por exemplo, quando voc√™ possui um DataFrame separado contendo as vari√°veis preditoras e outro com os r√≥tulos (labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing_imputed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"\n",
    "### üîç Conjunto de Treinamento (`train_set`)\n",
    "\n",
    "{train_set.head().to_markdown(index=False)}\n",
    "\n",
    "### üîç Conjunto de Teste (`test_set`)\n",
    "\n",
    "{test_set.head().to_markdown(index=False)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c31411",
   "metadata": {},
   "source": [
    "At√© agora, consideramos m√©todos de amostragem puramente aleat√≥rios. Isso geralmente funciona bem se seu conjunto de dados for suficientemente grande (especialmente em rela√ß√£o ao n√∫mero de atributos), mas caso contr√°rio, h√° o risco de introduzir um **vi√©s amostral significativo**.\n",
    "\n",
    "Por exemplo, quando os funcion√°rios de uma empresa de pesquisas decidem ligar para 1.000 pessoas para fazer algumas perguntas, eles n√£o escolhem essas 1.000 pessoas aleatoriamente de uma lista telef√¥nica. Eles tentam garantir que essas pessoas sejam **representativas da popula√ß√£o como um todo**, levando em conta as vari√°veis relevantes para a pesquisa.\n",
    "\n",
    "‚û°Ô∏è Por exemplo, a popula√ß√£o dos Estados Unidos √© composta por **51,1% de mulheres e 48,9% de homens**. Portanto, uma pesquisa bem conduzida nos EUA tentaria manter essa propor√ß√£o na amostra: **511 mulheres e 489 homens**, pelo menos se houver a possibilidade de que as respostas variem entre os g√™neros.\n",
    "\n",
    "Esse m√©todo √© chamado de **amostragem estratificada** (**stratified sampling**): a popula√ß√£o √© dividida em subgrupos homog√™neos, chamados de **estratos** (*strata*), e o n√∫mero adequado de inst√¢ncias √© selecionado de cada estrato para garantir que o conjunto de teste seja representativo da popula√ß√£o como um todo.\n",
    "\n",
    "‚ö†Ô∏è Se as pessoas respons√°veis pela pesquisa utilizassem apenas uma amostragem aleat√≥ria simples, haveria cerca de **10,7% de chance** de obter um conjunto de teste distorcido, com **menos de 48,5% de mulheres ou mais de 53,5% de mulheres**. De qualquer forma, os resultados da pesquisa provavelmente seriam **bastante enviesados**.\n",
    "\n",
    "---\n",
    "\n",
    "Agora, suponha que voc√™ tenha conversado com alguns especialistas que te informaram que a **renda mediana** (*median income*) √© um atributo muito importante para prever o pre√ßo mediano das casas.\n",
    "\n",
    "üìä Voc√™ pode querer garantir que o conjunto de teste seja representativo das diferentes faixas de renda presentes no dataset.\n",
    "\n",
    "Por√©m, como a renda mediana √© um atributo **cont√≠nuo e num√©rico**, √© necess√°rio primeiro transform√°-lo em uma vari√°vel categ√≥rica, criando **faixas de renda**.\n",
    "\n",
    "üîç Observando o histograma da renda mediana (como na Figura 2-8 do livro), percebe-se que a maioria dos valores est√° concentrada entre **1,5 e 6** (isto √©, entre **US$ 15.000 e US$ 60.000**), mas algumas rendas v√£o muito al√©m de 6.\n",
    "\n",
    "√â importante garantir que haja uma quantidade suficiente de inst√¢ncias em cada estrato. Caso contr√°rio, a estimativa da import√¢ncia de determinado estrato poder√° ser enviesada.\n",
    "\n",
    "‚ö†Ô∏è Isso significa que:\n",
    "- N√£o se deve criar estratos demais.\n",
    "- Cada estrato precisa ser grande o suficiente para gerar estat√≠sticas confi√°veis.\n",
    "\n",
    "---\n",
    "\n",
    "‚û°Ô∏è O c√≥digo a seguir utiliza a fun√ß√£o **`pd.cut()`** para criar um atributo de categoria de renda, com **cinco categorias** (rotuladas de 1 a 5):\n",
    "\n",
    "- Categoria 1: de **0 at√© 1.5** (ou seja, menos de **US$ 15.000**)\n",
    "- Categoria 2: de **1.5 at√© 3**\n",
    "- Categoria 3: de **3 at√© 4.5**\n",
    "- Categoria 4: de **4.5 at√© 6**\n",
    "- Categoria 5: de **6 em diante** (rendas muito altas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f35215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Cria uma nova coluna chamada 'income_cat' no DataFrame 'housing'.\n",
    "# Essa coluna representa categorias de renda, que s√£o derivadas da vari√°vel 'median_income'.\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],  # üî∏ Vari√°vel num√©rica que ser√° transformada em categorias.\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],  # üî∏ Define os intervalos (faixas) das categorias.\n",
    "    # ‚ñ™Ô∏è Intervalo 1: de 0 at√© 1.5\n",
    "    # ‚ñ™Ô∏è Intervalo 2: de 1.5 at√© 3.0\n",
    "    # ‚ñ™Ô∏è Intervalo 3: de 3.0 at√© 4.5\n",
    "    # ‚ñ™Ô∏è Intervalo 4: de 4.5 at√© 6.0\n",
    "    # ‚ñ™Ô∏è Intervalo 5: de 6.0 at√© infinito (rendas muito altas)\n",
    "    labels=[1, 2, 3, 4, 5]  # üî∏ R√≥tulos atribu√≠dos a cada categoria.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8819c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    housing[\"income_cat\"].value_counts()  # Conta quantas ocorr√™ncias h√° em cada categoria de renda.\n",
    "    .sort_index()                         # Organiza na ordem dos √≠ndices das categorias (de 1 a 5).\n",
    "    .plot.bar(                            # Cria um gr√°fico de barras.\n",
    "        rot=0,                            # Mant√©m os r√≥tulos do eixo X na horizontal (sem rota√ß√£o).\n",
    "        grid=True                          # Adiciona uma grade no fundo do gr√°fico.\n",
    "    )\n",
    ")\n",
    "\n",
    "# üîß Configura√ß√µes dos r√≥tulos e t√≠tulo do gr√°fico.\n",
    "plt.xlabel(\"Categoria de renda\")           # Define o r√≥tulo do eixo X.\n",
    "plt.ylabel(\"N√∫mero de distritos\")          # Define o r√≥tulo do eixo Y.\n",
    "plt.title(\"Distribui√ß√£o das categorias de renda\")  # Adiciona um t√≠tulo ao gr√°fico.\n",
    "\n",
    "# üî• Exibe o gr√°fico.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1712570",
   "metadata": {},
   "source": [
    "Agora estamos prontos para realizar uma **amostragem estratificada** baseada na categoria de renda. O Scikit-Learn oferece ferramentas espec√≠ficas para isso. Podemos utilizar o ``StratifiedShuffleSplit`` do Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a classe StratifiedShuffleSplit do m√≥dulo model_selection do sklearn\n",
    "# Esta classe realiza divis√µes estratificadas dos dados preservando a propor√ß√£o das classes\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Cria uma inst√¢ncia do StratifiedShuffleSplit com os seguintes par√¢metros:\n",
    "# - n_splits=10: gera 1 divis√£o dos dados\n",
    "# - test_size=0.2: 20% dos dados ser√£o usados para teste em cada divis√£o\n",
    "# - random_state=42: semente aleat√≥ria para reprodutibilidade\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop sobre cada divis√£o gerada pelo splitter:\n",
    "# - housing: DataFrame completo com os dados\n",
    "# - housing[\"income_cat\"]: coluna usada para estratifica√ß√£o (garante propor√ß√£o igual em treino/teste)\n",
    "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
    "    # Cria conjunto de treino usando os √≠ndices gerados\n",
    "    strat_train_set = housing.iloc[train_index]\n",
    "    \n",
    "    # Cria conjunto de teste usando os √≠ndices gerados\n",
    "    strat_test_set = housing.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ef94c",
   "metadata": {},
   "source": [
    "Podemos ver as propor√ß√µes da categoria de renda no conjunto de testes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2591c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1606cf",
   "metadata": {},
   "source": [
    "### An√°lise Comparativa de Amostragem Estratificada vs Aleat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    \"\"\"Calcula as propor√ß√µes das categorias de renda em um conjunto de dados.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        data (DataFrame): Conjunto de dados contendo a coluna 'income_cat'\n",
    "        \n",
    "    Retorna:\n",
    "        Series: Propor√ß√µes de cada categoria de renda\n",
    "    \"\"\"\n",
    "    return data[\"income_cat\"].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide os dados de forma aleat√≥ria (n√£o estratificada)\n",
    "# test_size=0.2 ‚Üí 20% para teste, 80% para treino\n",
    "# random_state=42 ‚Üí garante reprodutibilidade\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cria DataFrame comparativo das propor√ß√µes:\n",
    "# - Overall: Propor√ß√µes no dataset completo\n",
    "# - Stratified: Propor√ß√µes no teste estratificado\n",
    "# - Random: Propor√ß√µes no teste aleat√≥rio\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "\n",
    "# Calcula os erros percentuais das amostragens:\n",
    "# - Rand. %error: Diferen√ßa percentual da amostragem aleat√≥ria\n",
    "# - Strat. %error: Diferen√ßa percentual da amostragem estratificada\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "# Exibe a tabela comparativa\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea007b9",
   "metadata": {},
   "source": [
    "√â poss√≠vel medir as propor√ß√µes das categorias de renda no conjunto de dados completo. A figura acima compara essas propor√ß√µes em tr√™s cen√°rios: (1) no dataset original, (2) no conjunto de teste gerado por amostragem estratificada e (3) no conjunto de teste criado com divis√£o puramente aleat√≥ria. Os resultados mostram que a **amostragem estratificada** preserva propor√ß√µes quase id√™nticas √†s do dataset original, enquanto a divis√£o aleat√≥ria apresenta distor√ß√µes significativas (*skew*). Isso demonstra a superioridade da estratifica√ß√£o para manter a representatividade estat√≠stica, especialmente em an√°lises onde o balanceamento das categorias √© cr√≠tico.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3a5e1",
   "metadata": {},
   "source": [
    "## Atividade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1d4b1",
   "metadata": {},
   "source": [
    "**Exerc√≠cio de Pr√©-processamento**  \n",
    "Voc√™ ir√°:  \n",
    "Baixar o dataset Iris diretamente do reposit√≥rio online usando `fetch_openml`:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Carregando dataset online (formato TGZ impl√≠cito)\n",
    "iris = fetch_openml('iris', version=1, as_frame=True)\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "```\n",
    "**Quest√µes:**\n",
    "1. Aplicar **escalonamento** (√† escolha: padroniza√ß√£o ou normaliza√ß√£o)  \n",
    "2. Dividir em treino/teste com **dois m√©todos** (aleat√≥rio e estratificado)  \n",
    "3. Verifique as propor√ß√µes das classes em cada conjunto  \n",
    "\n",
    "**Dica:** Compare os resultados dos dois m√©todos de divis√£o para entender o impacto da estratifica√ß√£o em dados balanceados. O relat√≥rio final deve mostrar as m√©tricas de propor√ß√£o antes/depois do processamento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
